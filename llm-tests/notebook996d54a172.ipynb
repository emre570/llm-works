{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 2\n",
      "GPU 0: NVIDIA GeForce RTX 4090\n",
      "GPU 1: NVIDIA GeForce RTX 4090\n",
      "Initial GPU memory usage:\n",
      "GPU 0 memory allocated: 0.0 MB\n",
      "GPU 0 memory reserved: 0.0 MB\n",
      "GPU 1 memory allocated: 0.0 MB\n",
      "GPU 1 memory reserved: 0.0 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import multiprocessing\n",
    "import os\n",
    "\n",
    "# Set multiprocessing start method for CUDA compatibility\n",
    "multiprocessing.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "# Set CUDA_VISIBLE_DEVICES for specific GPUs if needed\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"  # Use GPUs 0 and 1\n",
    "\n",
    "def device_count():\n",
    "    print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "device_count()\n",
    "\n",
    "# Check GPU memory before starting\n",
    "print(\"Initial GPU memory usage:\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i} memory allocated: {torch.cuda.memory_allocated(i) / 1e6} MB\")\n",
    "    print(f\"GPU {i} memory reserved: {torch.cuda.memory_reserved(i) / 1e6} MB\")\n",
    "\n",
    "# Clear CUDA cache\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: 0\n"
     ]
    }
   ],
   "source": [
    "from accelerate import PartialState\n",
    "\n",
    "def get_device():\n",
    "    device_str = PartialState().process_index\n",
    "    print(f\"Using device: {device_str}\")\n",
    "    return device_str\n",
    "\n",
    "device_str = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T12:17:21.277429Z",
     "iopub.status.busy": "2024-12-14T12:17:21.276799Z",
     "iopub.status.idle": "2024-12-14T12:17:21.281096Z",
     "shell.execute_reply": "2024-12-14T12:17:21.280162Z",
     "shell.execute_reply.started": "2024-12-14T12:17:21.277396Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#from huggingface_hub import notebook_login\n",
    "#notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T12:17:24.219417Z",
     "iopub.status.busy": "2024-12-14T12:17:24.218999Z",
     "iopub.status.idle": "2024-12-14T12:17:37.663536Z",
     "shell.execute_reply": "2024-12-14T12:17:37.662822Z",
     "shell.execute_reply.started": "2024-12-14T12:17:24.219388Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import LoraConfig\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T12:17:39.838121Z",
     "iopub.status.busy": "2024-12-14T12:17:39.837471Z",
     "iopub.status.idle": "2024-12-14T12:18:29.800233Z",
     "shell.execute_reply": "2024-12-14T12:18:29.799509Z",
     "shell.execute_reply.started": "2024-12-14T12:17:39.838090Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec689a5d4fd422fbd2c4bd7bb962615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "modelName = \"google/gemma-2-2b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelName, token=hf_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(modelName, \n",
    "                                             quantization_config=bnb_config, \n",
    "                                             device_map=\"auto\",\n",
    "                                             #device_map={\"\":device_str}, \n",
    "                                             token=hf_token)\n",
    "\n",
    "#print(f\"Device map for training: {model.device_map}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device map of model layers:\n",
      "model.embed_tokens.weight is on cuda:0\n",
      "model.layers.0.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.0.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.0.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.0.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.0.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.0.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.0.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.0.input_layernorm.weight is on cuda:0\n",
      "model.layers.0.pre_feedforward_layernorm.weight is on cuda:0\n",
      "model.layers.0.post_feedforward_layernorm.weight is on cuda:0\n",
      "model.layers.0.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.1.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.1.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.1.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.1.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.1.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.1.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.1.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.1.input_layernorm.weight is on cuda:0\n",
      "model.layers.1.pre_feedforward_layernorm.weight is on cuda:0\n",
      "model.layers.1.post_feedforward_layernorm.weight is on cuda:0\n",
      "model.layers.1.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.2.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.2.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.2.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.2.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.2.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.2.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.2.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.2.input_layernorm.weight is on cuda:0\n",
      "model.layers.2.pre_feedforward_layernorm.weight is on cuda:0\n",
      "model.layers.2.post_feedforward_layernorm.weight is on cuda:0\n",
      "model.layers.2.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.3.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.3.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.3.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.3.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.3.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.3.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.3.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.3.input_layernorm.weight is on cuda:0\n",
      "model.layers.3.pre_feedforward_layernorm.weight is on cuda:0\n",
      "model.layers.3.post_feedforward_layernorm.weight is on cuda:0\n",
      "model.layers.3.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.4.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.4.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.4.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.4.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.4.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.4.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.4.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.4.input_layernorm.weight is on cuda:0\n",
      "model.layers.4.pre_feedforward_layernorm.weight is on cuda:0\n",
      "model.layers.4.post_feedforward_layernorm.weight is on cuda:0\n",
      "model.layers.4.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.5.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.5.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.5.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.5.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.5.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.5.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.5.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.5.input_layernorm.weight is on cuda:0\n",
      "model.layers.5.pre_feedforward_layernorm.weight is on cuda:0\n",
      "model.layers.5.post_feedforward_layernorm.weight is on cuda:0\n",
      "model.layers.5.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.6.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.6.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.6.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.6.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.6.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.6.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.6.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.6.input_layernorm.weight is on cuda:0\n",
      "model.layers.6.pre_feedforward_layernorm.weight is on cuda:0\n",
      "model.layers.6.post_feedforward_layernorm.weight is on cuda:0\n",
      "model.layers.6.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.7.self_attn.q_proj.weight is on cuda:0\n",
      "model.layers.7.self_attn.k_proj.weight is on cuda:0\n",
      "model.layers.7.self_attn.v_proj.weight is on cuda:0\n",
      "model.layers.7.self_attn.o_proj.weight is on cuda:0\n",
      "model.layers.7.mlp.gate_proj.weight is on cuda:0\n",
      "model.layers.7.mlp.up_proj.weight is on cuda:0\n",
      "model.layers.7.mlp.down_proj.weight is on cuda:0\n",
      "model.layers.7.input_layernorm.weight is on cuda:0\n",
      "model.layers.7.pre_feedforward_layernorm.weight is on cuda:0\n",
      "model.layers.7.post_feedforward_layernorm.weight is on cuda:0\n",
      "model.layers.7.post_attention_layernorm.weight is on cuda:0\n",
      "model.layers.8.self_attn.q_proj.weight is on cuda:1\n",
      "model.layers.8.self_attn.k_proj.weight is on cuda:1\n",
      "model.layers.8.self_attn.v_proj.weight is on cuda:1\n",
      "model.layers.8.self_attn.o_proj.weight is on cuda:1\n",
      "model.layers.8.mlp.gate_proj.weight is on cuda:1\n",
      "model.layers.8.mlp.up_proj.weight is on cuda:1\n",
      "model.layers.8.mlp.down_proj.weight is on cuda:1\n",
      "model.layers.8.input_layernorm.weight is on cuda:1\n",
      "model.layers.8.pre_feedforward_layernorm.weight is on cuda:1\n",
      "model.layers.8.post_feedforward_layernorm.weight is on cuda:1\n",
      "model.layers.8.post_attention_layernorm.weight is on cuda:1\n",
      "model.layers.9.self_attn.q_proj.weight is on cuda:1\n",
      "model.layers.9.self_attn.k_proj.weight is on cuda:1\n",
      "model.layers.9.self_attn.v_proj.weight is on cuda:1\n",
      "model.layers.9.self_attn.o_proj.weight is on cuda:1\n",
      "model.layers.9.mlp.gate_proj.weight is on cuda:1\n",
      "model.layers.9.mlp.up_proj.weight is on cuda:1\n",
      "model.layers.9.mlp.down_proj.weight is on cuda:1\n",
      "model.layers.9.input_layernorm.weight is on cuda:1\n",
      "model.layers.9.pre_feedforward_layernorm.weight is on cuda:1\n",
      "model.layers.9.post_feedforward_layernorm.weight is on cuda:1\n",
      "model.layers.9.post_attention_layernorm.weight is on cuda:1\n",
      "model.layers.10.self_attn.q_proj.weight is on cuda:1\n",
      "model.layers.10.self_attn.k_proj.weight is on cuda:1\n",
      "model.layers.10.self_attn.v_proj.weight is on cuda:1\n",
      "model.layers.10.self_attn.o_proj.weight is on cuda:1\n",
      "model.layers.10.mlp.gate_proj.weight is on cuda:1\n",
      "model.layers.10.mlp.up_proj.weight is on cuda:1\n",
      "model.layers.10.mlp.down_proj.weight is on cuda:1\n",
      "model.layers.10.input_layernorm.weight is on cuda:1\n",
      "model.layers.10.pre_feedforward_layernorm.weight is on cuda:1\n",
      "model.layers.10.post_feedforward_layernorm.weight is on cuda:1\n",
      "model.layers.10.post_attention_layernorm.weight is on cuda:1\n",
      "model.layers.11.self_attn.q_proj.weight is on cuda:1\n",
      "model.layers.11.self_attn.k_proj.weight is on cuda:1\n",
      "model.layers.11.self_attn.v_proj.weight is on cuda:1\n",
      "model.layers.11.self_attn.o_proj.weight is on cuda:1\n",
      "model.layers.11.mlp.gate_proj.weight is on cuda:1\n",
      "model.layers.11.mlp.up_proj.weight is on cuda:1\n",
      "model.layers.11.mlp.down_proj.weight is on cuda:1\n",
      "model.layers.11.input_layernorm.weight is on cuda:1\n",
      "model.layers.11.pre_feedforward_layernorm.weight is on cuda:1\n",
      "model.layers.11.post_feedforward_layernorm.weight is on cuda:1\n",
      "model.layers.11.post_attention_layernorm.weight is on cuda:1\n",
      "model.layers.12.self_attn.q_proj.weight is on cuda:1\n",
      "model.layers.12.self_attn.k_proj.weight is on cuda:1\n",
      "model.layers.12.self_attn.v_proj.weight is on cuda:1\n",
      "model.layers.12.self_attn.o_proj.weight is on cuda:1\n",
      "model.layers.12.mlp.gate_proj.weight is on cuda:1\n",
      "model.layers.12.mlp.up_proj.weight is on cuda:1\n",
      "model.layers.12.mlp.down_proj.weight is on cuda:1\n",
      "model.layers.12.input_layernorm.weight is on cuda:1\n",
      "model.layers.12.pre_feedforward_layernorm.weight is on cuda:1\n",
      "model.layers.12.post_feedforward_layernorm.weight is on cuda:1\n",
      "model.layers.12.post_attention_layernorm.weight is on cuda:1\n",
      "model.layers.13.self_attn.q_proj.weight is on cuda:1\n",
      "model.layers.13.self_attn.k_proj.weight is on cuda:1\n",
      "model.layers.13.self_attn.v_proj.weight is on cuda:1\n",
      "model.layers.13.self_attn.o_proj.weight is on cuda:1\n",
      "model.layers.13.mlp.gate_proj.weight is on cuda:1\n",
      "model.layers.13.mlp.up_proj.weight is on cuda:1\n",
      "model.layers.13.mlp.down_proj.weight is on cuda:1\n",
      "model.layers.13.input_layernorm.weight is on cuda:1\n",
      "model.layers.13.pre_feedforward_layernorm.weight is on cuda:1\n",
      "model.layers.13.post_feedforward_layernorm.weight is on cuda:1\n",
      "model.layers.13.post_attention_layernorm.weight is on cuda:1\n",
      "model.layers.14.self_attn.q_proj.weight is on cuda:1\n",
      "model.layers.14.self_attn.k_proj.weight is on cuda:1\n",
      "model.layers.14.self_attn.v_proj.weight is on cuda:1\n",
      "model.layers.14.self_attn.o_proj.weight is on cuda:1\n",
      "model.layers.14.mlp.gate_proj.weight is on cuda:1\n",
      "model.layers.14.mlp.up_proj.weight is on cuda:1\n",
      "model.layers.14.mlp.down_proj.weight is on cuda:1\n",
      "model.layers.14.input_layernorm.weight is on cuda:1\n",
      "model.layers.14.pre_feedforward_layernorm.weight is on cuda:1\n",
      "model.layers.14.post_feedforward_layernorm.weight is on cuda:1\n",
      "model.layers.14.post_attention_layernorm.weight is on cuda:1\n",
      "model.layers.15.self_attn.q_proj.weight is on cuda:1\n",
      "model.layers.15.self_attn.k_proj.weight is on cuda:1\n",
      "model.layers.15.self_attn.v_proj.weight is on cuda:1\n",
      "model.layers.15.self_attn.o_proj.weight is on cuda:1\n",
      "model.layers.15.mlp.gate_proj.weight is on cuda:1\n",
      "model.layers.15.mlp.up_proj.weight is on cuda:1\n",
      "model.layers.15.mlp.down_proj.weight is on cuda:1\n",
      "model.layers.15.input_layernorm.weight is on cuda:1\n",
      "model.layers.15.pre_feedforward_layernorm.weight is on cuda:1\n",
      "model.layers.15.post_feedforward_layernorm.weight is on cuda:1\n",
      "model.layers.15.post_attention_layernorm.weight is on cuda:1\n",
      "model.layers.16.self_attn.q_proj.weight is on cuda:1\n",
      "model.layers.16.self_attn.k_proj.weight is on cuda:1\n",
      "model.layers.16.self_attn.v_proj.weight is on cuda:1\n",
      "model.layers.16.self_attn.o_proj.weight is on cuda:1\n",
      "model.layers.16.mlp.gate_proj.weight is on cuda:1\n",
      "model.layers.16.mlp.up_proj.weight is on cuda:1\n",
      "model.layers.16.mlp.down_proj.weight is on cuda:1\n",
      "model.layers.16.input_layernorm.weight is on cuda:1\n",
      "model.layers.16.pre_feedforward_layernorm.weight is on cuda:1\n",
      "model.layers.16.post_feedforward_layernorm.weight is on cuda:1\n",
      "model.layers.16.post_attention_layernorm.weight is on cuda:1\n",
      "model.layers.17.self_attn.q_proj.weight is on cuda:1\n",
      "model.layers.17.self_attn.k_proj.weight is on cuda:1\n",
      "model.layers.17.self_attn.v_proj.weight is on cuda:1\n",
      "model.layers.17.self_attn.o_proj.weight is on cuda:1\n",
      "model.layers.17.mlp.gate_proj.weight is on cuda:1\n",
      "model.layers.17.mlp.up_proj.weight is on cuda:1\n",
      "model.layers.17.mlp.down_proj.weight is on cuda:1\n",
      "model.layers.17.input_layernorm.weight is on cuda:1\n",
      "model.layers.17.pre_feedforward_layernorm.weight is on cuda:1\n",
      "model.layers.17.post_feedforward_layernorm.weight is on cuda:1\n",
      "model.layers.17.post_attention_layernorm.weight is on cuda:1\n",
      "model.layers.18.self_attn.q_proj.weight is on cuda:1\n",
      "model.layers.18.self_attn.k_proj.weight is on cuda:1\n",
      "model.layers.18.self_attn.v_proj.weight is on cuda:1\n",
      "model.layers.18.self_attn.o_proj.weight is on cuda:1\n",
      "model.layers.18.mlp.gate_proj.weight is on cuda:1\n",
      "model.layers.18.mlp.up_proj.weight is on cuda:1\n",
      "model.layers.18.mlp.down_proj.weight is on cuda:1\n",
      "model.layers.18.input_layernorm.weight is on cuda:1\n",
      "model.layers.18.pre_feedforward_layernorm.weight is on cuda:1\n",
      "model.layers.18.post_feedforward_layernorm.weight is on cuda:1\n",
      "model.layers.18.post_attention_layernorm.weight is on cuda:1\n",
      "model.layers.19.self_attn.q_proj.weight is on cuda:1\n",
      "model.layers.19.self_attn.k_proj.weight is on cuda:1\n",
      "model.layers.19.self_attn.v_proj.weight is on cuda:1\n",
      "model.layers.19.self_attn.o_proj.weight is on cuda:1\n",
      "model.layers.19.mlp.gate_proj.weight is on cuda:1\n",
      "model.layers.19.mlp.up_proj.weight is on cuda:1\n",
      "model.layers.19.mlp.down_proj.weight is on cuda:1\n",
      "model.layers.19.input_layernorm.weight is on cuda:1\n",
      "model.layers.19.pre_feedforward_layernorm.weight is on cuda:1\n",
      "model.layers.19.post_feedforward_layernorm.weight is on cuda:1\n",
      "model.layers.19.post_attention_layernorm.weight is on cuda:1\n",
      "model.layers.20.self_attn.q_proj.weight is on cuda:1\n",
      "model.layers.20.self_attn.k_proj.weight is on cuda:1\n",
      "model.layers.20.self_attn.v_proj.weight is on cuda:1\n",
      "model.layers.20.self_attn.o_proj.weight is on cuda:1\n",
      "model.layers.20.mlp.gate_proj.weight is on cuda:1\n",
      "model.layers.20.mlp.up_proj.weight is on cuda:1\n",
      "model.layers.20.mlp.down_proj.weight is on cuda:1\n",
      "model.layers.20.input_layernorm.weight is on cuda:1\n",
      "model.layers.20.pre_feedforward_layernorm.weight is on cuda:1\n",
      "model.layers.20.post_feedforward_layernorm.weight is on cuda:1\n",
      "model.layers.20.post_attention_layernorm.weight is on cuda:1\n",
      "model.layers.21.self_attn.q_proj.weight is on cuda:1\n",
      "model.layers.21.self_attn.k_proj.weight is on cuda:1\n",
      "model.layers.21.self_attn.v_proj.weight is on cuda:1\n",
      "model.layers.21.self_attn.o_proj.weight is on cuda:1\n",
      "model.layers.21.mlp.gate_proj.weight is on cuda:1\n",
      "model.layers.21.mlp.up_proj.weight is on cuda:1\n",
      "model.layers.21.mlp.down_proj.weight is on cuda:1\n",
      "model.layers.21.input_layernorm.weight is on cuda:1\n",
      "model.layers.21.pre_feedforward_layernorm.weight is on cuda:1\n",
      "model.layers.21.post_feedforward_layernorm.weight is on cuda:1\n",
      "model.layers.21.post_attention_layernorm.weight is on cuda:1\n",
      "model.layers.22.self_attn.q_proj.weight is on cuda:1\n",
      "model.layers.22.self_attn.k_proj.weight is on cuda:1\n",
      "model.layers.22.self_attn.v_proj.weight is on cuda:1\n",
      "model.layers.22.self_attn.o_proj.weight is on cuda:1\n",
      "model.layers.22.mlp.gate_proj.weight is on cuda:1\n",
      "model.layers.22.mlp.up_proj.weight is on cuda:1\n",
      "model.layers.22.mlp.down_proj.weight is on cuda:1\n",
      "model.layers.22.input_layernorm.weight is on cuda:1\n",
      "model.layers.22.pre_feedforward_layernorm.weight is on cuda:1\n",
      "model.layers.22.post_feedforward_layernorm.weight is on cuda:1\n",
      "model.layers.22.post_attention_layernorm.weight is on cuda:1\n",
      "model.layers.23.self_attn.q_proj.weight is on cuda:1\n",
      "model.layers.23.self_attn.k_proj.weight is on cuda:1\n",
      "model.layers.23.self_attn.v_proj.weight is on cuda:1\n",
      "model.layers.23.self_attn.o_proj.weight is on cuda:1\n",
      "model.layers.23.mlp.gate_proj.weight is on cuda:1\n",
      "model.layers.23.mlp.up_proj.weight is on cuda:1\n",
      "model.layers.23.mlp.down_proj.weight is on cuda:1\n",
      "model.layers.23.input_layernorm.weight is on cuda:1\n",
      "model.layers.23.pre_feedforward_layernorm.weight is on cuda:1\n",
      "model.layers.23.post_feedforward_layernorm.weight is on cuda:1\n",
      "model.layers.23.post_attention_layernorm.weight is on cuda:1\n",
      "model.layers.24.self_attn.q_proj.weight is on cuda:1\n",
      "model.layers.24.self_attn.k_proj.weight is on cuda:1\n",
      "model.layers.24.self_attn.v_proj.weight is on cuda:1\n",
      "model.layers.24.self_attn.o_proj.weight is on cuda:1\n",
      "model.layers.24.mlp.gate_proj.weight is on cuda:1\n",
      "model.layers.24.mlp.up_proj.weight is on cuda:1\n",
      "model.layers.24.mlp.down_proj.weight is on cuda:1\n",
      "model.layers.24.input_layernorm.weight is on cuda:1\n",
      "model.layers.24.pre_feedforward_layernorm.weight is on cuda:1\n",
      "model.layers.24.post_feedforward_layernorm.weight is on cuda:1\n",
      "model.layers.24.post_attention_layernorm.weight is on cuda:1\n",
      "model.layers.25.self_attn.q_proj.weight is on cuda:1\n",
      "model.layers.25.self_attn.k_proj.weight is on cuda:1\n",
      "model.layers.25.self_attn.v_proj.weight is on cuda:1\n",
      "model.layers.25.self_attn.o_proj.weight is on cuda:1\n",
      "model.layers.25.mlp.gate_proj.weight is on cuda:1\n",
      "model.layers.25.mlp.up_proj.weight is on cuda:1\n",
      "model.layers.25.mlp.down_proj.weight is on cuda:1\n",
      "model.layers.25.input_layernorm.weight is on cuda:1\n",
      "model.layers.25.pre_feedforward_layernorm.weight is on cuda:1\n",
      "model.layers.25.post_feedforward_layernorm.weight is on cuda:1\n",
      "model.layers.25.post_attention_layernorm.weight is on cuda:1\n",
      "model.norm.weight is on cuda:1\n"
     ]
    }
   ],
   "source": [
    "print(\"Device map of model layers:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name} is on {param.device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T12:18:40.869795Z",
     "iopub.status.busy": "2024-12-14T12:18:40.869244Z",
     "iopub.status.idle": "2024-12-14T12:18:43.418800Z",
     "shell.execute_reply": "2024-12-14T12:18:43.418059Z",
     "shell.execute_reply.started": "2024-12-14T12:18:40.869764Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['input', 'output', 'instruction'],\n",
       "     num_rows: 82353\n",
       " }),\n",
       " {'input': '',\n",
       "  'output': \"Fransa'nın başkenti Paris'tir.\",\n",
       "  'instruction': \"Fransa'nın başkenti nedir?\"})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"myzens/alpaca-turkish-combined\", split=\"train\")\n",
    "dataset, dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T12:18:44.923956Z",
     "iopub.status.busy": "2024-12-14T12:18:44.923180Z",
     "iopub.status.idle": "2024-12-14T12:18:44.929175Z",
     "shell.execute_reply": "2024-12-14T12:18:44.928050Z",
     "shell.execute_reply.started": "2024-12-14T12:18:44.923925Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start_of_turn>user\\n{}: {}<end_of_turn>\\n<start_of_turn>model\\n{}<end_of_turn>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_prompt = \"\"\"<start_of_turn>user\n",
    "{}: {}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "{}<end_of_turn>\"\"\"\n",
    "gemma_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T12:18:46.543848Z",
     "iopub.status.busy": "2024-12-14T12:18:46.543487Z",
     "iopub.status.idle": "2024-12-14T12:18:46.549666Z",
     "shell.execute_reply": "2024-12-14T12:18:46.548724Z",
     "shell.execute_reply.started": "2024-12-14T12:18:46.543819Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<eos>', '<pad>')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eos_token = tokenizer.eos_token\n",
    "pad_token = tokenizer.pad_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "eos_token, pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T12:18:47.877321Z",
     "iopub.status.busy": "2024-12-14T12:18:47.876620Z",
     "iopub.status.idle": "2024-12-14T12:18:47.882241Z",
     "shell.execute_reply": "2024-12-14T12:18:47.881338Z",
     "shell.execute_reply.started": "2024-12-14T12:18:47.877287Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        text = gemma_prompt.format(instruction, input, output) + eos_token\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T12:18:49.996836Z",
     "iopub.status.busy": "2024-12-14T12:18:49.996041Z",
     "iopub.status.idle": "2024-12-14T12:18:50.975247Z",
     "shell.execute_reply": "2024-12-14T12:18:50.974505Z",
     "shell.execute_reply.started": "2024-12-14T12:18:49.996805Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'output', 'instruction', 'text'],\n",
       "    num_rows: 82353\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.map(formatting_prompts_func, batched = True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T12:18:52.788891Z",
     "iopub.status.busy": "2024-12-14T12:18:52.788565Z",
     "iopub.status.idle": "2024-12-14T12:18:53.019576Z",
     "shell.execute_reply": "2024-12-14T12:18:53.018612Z",
     "shell.execute_reply.started": "2024-12-14T12:18:52.788863Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start_of_turn>user\n",
      "Tek farklı olanı belirleyin.: Twitter, Instagram, Telegram<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Telegram<end_of_turn><eos>\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"text\"][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset...\n",
      "Dataset tokenized: {'input': '', 'output': \"Fransa'nın başkenti Paris'tir.\", 'instruction': \"Fransa'nın başkenti nedir?\", 'input_ids': [2, 106, 1645, 108, 21727, 29541, 235303, 68749, 20074, 235273, 1077, 91278, 7846, 235248, 107, 108, 106, 2516, 108, 21727, 29541, 235303, 68749, 20074, 235273, 1077, 7127, 235303, 6651, 235265, 107, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [2, 106, 1645, 108, 21727, 29541, 235303, 68749, 20074, 235273, 1077, 91278, 7846, 235248, 107, 108, 106, 2516, 108, 21727, 29541, 235303, 68749, 20074, 235273, 1077, 7127, 235303, 6651, 235265, 107, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # Labels are identical to input_ids for causal language modeling\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    return tokenized\n",
    "\n",
    "print(\"Tokenizing dataset...\")\n",
    "dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "print(\"Dataset tokenized:\", dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T12:18:57.268796Z",
     "iopub.status.busy": "2024-12-14T12:18:57.268440Z",
     "iopub.status.idle": "2024-12-14T12:18:58.352560Z",
     "shell.execute_reply": "2024-12-14T12:18:58.351444Z",
     "shell.execute_reply.started": "2024-12-14T12:18:57.268764Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    per_device_train_batch_size=4,  # Lowered batch size for 2B model in Jupyter\n",
    "    gradient_accumulation_steps=4,  # Higher accumulation to simulate larger batch size\n",
    "    warmup_steps=30,\n",
    "    max_steps=500,\n",
    "    #num_train_epochs=3,\n",
    "    gradient_checkpointing=True,\n",
    "    learning_rate=1e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    output_dir=\"outputs\",    \n",
    "    report_to=\"none\",\n",
    "    ddp_find_unused_parameters=False,  # For DDP compatibility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-14T12:19:14.467581Z",
     "iopub.status.busy": "2024-12-14T12:19:14.466833Z",
     "iopub.status.idle": "2024-12-14T12:19:54.891017Z",
     "shell.execute_reply": "2024-12-14T12:19:54.889645Z",
     "shell.execute_reply.started": "2024-12-14T12:19:14.467549Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emre570/Documents/llm-works/llmenv/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '0.13.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/emre570/Documents/llm-works/llmenv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-15 16:25:11,011] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58feeae7c71d4fdb9e8372597770b871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/emre570/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 14.5238, 'grad_norm': 20.342395782470703, 'learning_rate': 2.3333333333333336e-05, 'epoch': 0.0}\n",
      "{'loss': 4.268, 'grad_norm': 7.5723795890808105, 'learning_rate': 5.666666666666667e-05, 'epoch': 0.0}\n",
      "{'loss': 3.0381, 'grad_norm': 5.141354560852051, 'learning_rate': 9e-05, 'epoch': 0.01}\n",
      "{'loss': 1.5589, 'grad_norm': 27.26141357421875, 'learning_rate': 9.851063829787235e-05, 'epoch': 0.01}\n",
      "{'loss': 1.7459, 'grad_norm': 17.041549682617188, 'learning_rate': 9.638297872340426e-05, 'epoch': 0.01}\n",
      "{'loss': 0.7791, 'grad_norm': 5.4085283279418945, 'learning_rate': 9.425531914893617e-05, 'epoch': 0.01}\n",
      "{'loss': 0.6331, 'grad_norm': 3.4994218349456787, 'learning_rate': 9.212765957446809e-05, 'epoch': 0.01}\n",
      "{'loss': 0.5889, 'grad_norm': 1.4355807304382324, 'learning_rate': 9e-05, 'epoch': 0.02}\n",
      "{'loss': 0.5901, 'grad_norm': 10.499778747558594, 'learning_rate': 8.787234042553192e-05, 'epoch': 0.02}\n",
      "{'loss': 0.5388, 'grad_norm': 5.884941101074219, 'learning_rate': 8.574468085106383e-05, 'epoch': 0.02}\n",
      "{'loss': 0.5259, 'grad_norm': 4.7235798835754395, 'learning_rate': 8.361702127659576e-05, 'epoch': 0.02}\n",
      "{'loss': 0.5261, 'grad_norm': 2.89433217048645, 'learning_rate': 8.148936170212766e-05, 'epoch': 0.02}\n",
      "{'loss': 0.5483, 'grad_norm': 9.383684158325195, 'learning_rate': 7.936170212765958e-05, 'epoch': 0.03}\n",
      "{'loss': 0.5434, 'grad_norm': 2.682504653930664, 'learning_rate': 7.723404255319149e-05, 'epoch': 0.03}\n",
      "{'loss': 0.5297, 'grad_norm': 4.118009090423584, 'learning_rate': 7.510638297872341e-05, 'epoch': 0.03}\n",
      "{'loss': 0.5165, 'grad_norm': 3.430727958679199, 'learning_rate': 7.297872340425533e-05, 'epoch': 0.03}\n",
      "{'loss': 0.5278, 'grad_norm': 4.599996566772461, 'learning_rate': 7.085106382978723e-05, 'epoch': 0.03}\n",
      "{'loss': 0.5144, 'grad_norm': 3.0955071449279785, 'learning_rate': 6.872340425531916e-05, 'epoch': 0.03}\n",
      "{'loss': 0.5133, 'grad_norm': 4.167599201202393, 'learning_rate': 6.659574468085106e-05, 'epoch': 0.04}\n",
      "{'loss': 0.5175, 'grad_norm': 2.9216606616973877, 'learning_rate': 6.446808510638298e-05, 'epoch': 0.04}\n",
      "{'loss': 0.4919, 'grad_norm': 3.1490986347198486, 'learning_rate': 6.23404255319149e-05, 'epoch': 0.04}\n",
      "{'loss': 0.479, 'grad_norm': 2.9498512744903564, 'learning_rate': 6.0212765957446814e-05, 'epoch': 0.04}\n",
      "{'loss': 0.5122, 'grad_norm': 3.0507755279541016, 'learning_rate': 5.8085106382978724e-05, 'epoch': 0.04}\n",
      "{'loss': 0.4881, 'grad_norm': 2.8418092727661133, 'learning_rate': 5.595744680851064e-05, 'epoch': 0.05}\n",
      "{'loss': 0.5106, 'grad_norm': 2.7235121726989746, 'learning_rate': 5.3829787234042564e-05, 'epoch': 0.05}\n",
      "{'loss': 0.4928, 'grad_norm': 2.8138298988342285, 'learning_rate': 5.1702127659574474e-05, 'epoch': 0.05}\n",
      "{'loss': 0.4918, 'grad_norm': 2.428708076477051, 'learning_rate': 4.9574468085106384e-05, 'epoch': 0.05}\n",
      "{'loss': 0.5046, 'grad_norm': 2.6130990982055664, 'learning_rate': 4.74468085106383e-05, 'epoch': 0.05}\n",
      "{'loss': 0.5128, 'grad_norm': 2.23984694480896, 'learning_rate': 4.531914893617021e-05, 'epoch': 0.06}\n",
      "{'loss': 0.4817, 'grad_norm': 2.0407068729400635, 'learning_rate': 4.319148936170213e-05, 'epoch': 0.06}\n",
      "{'loss': 0.4998, 'grad_norm': 2.2254486083984375, 'learning_rate': 4.1063829787234045e-05, 'epoch': 0.06}\n",
      "{'loss': 0.4773, 'grad_norm': 2.0897324085235596, 'learning_rate': 3.893617021276596e-05, 'epoch': 0.06}\n",
      "{'loss': 0.4759, 'grad_norm': 1.9421732425689697, 'learning_rate': 3.680851063829787e-05, 'epoch': 0.06}\n",
      "{'loss': 0.4956, 'grad_norm': 1.7484300136566162, 'learning_rate': 3.468085106382979e-05, 'epoch': 0.07}\n",
      "{'loss': 0.4752, 'grad_norm': 1.5975407361984253, 'learning_rate': 3.2553191489361705e-05, 'epoch': 0.07}\n",
      "{'loss': 0.486, 'grad_norm': 1.6989784240722656, 'learning_rate': 3.042553191489362e-05, 'epoch': 0.07}\n",
      "{'loss': 0.4853, 'grad_norm': 1.429490327835083, 'learning_rate': 2.8297872340425536e-05, 'epoch': 0.07}\n",
      "{'loss': 0.4975, 'grad_norm': 1.3645023107528687, 'learning_rate': 2.6170212765957446e-05, 'epoch': 0.07}\n",
      "{'loss': 0.4673, 'grad_norm': 1.2150671482086182, 'learning_rate': 2.4042553191489362e-05, 'epoch': 0.08}\n",
      "{'loss': 0.4786, 'grad_norm': 1.093208909034729, 'learning_rate': 2.191489361702128e-05, 'epoch': 0.08}\n",
      "{'loss': 0.5043, 'grad_norm': 1.1067315340042114, 'learning_rate': 1.9787234042553193e-05, 'epoch': 0.08}\n",
      "{'loss': 0.4793, 'grad_norm': 0.9707982540130615, 'learning_rate': 1.7659574468085106e-05, 'epoch': 0.08}\n",
      "{'loss': 0.4852, 'grad_norm': 1.0735057592391968, 'learning_rate': 1.5531914893617023e-05, 'epoch': 0.08}\n",
      "{'loss': 0.4858, 'grad_norm': 0.2591632008552551, 'learning_rate': 1.3404255319148936e-05, 'epoch': 0.09}\n",
      "{'loss': 0.4904, 'grad_norm': 0.2823965549468994, 'learning_rate': 1.1276595744680851e-05, 'epoch': 0.09}\n",
      "{'loss': 0.4663, 'grad_norm': 0.34088894724845886, 'learning_rate': 9.148936170212767e-06, 'epoch': 0.09}\n",
      "{'loss': 0.4602, 'grad_norm': 0.245230033993721, 'learning_rate': 7.021276595744682e-06, 'epoch': 0.09}\n",
      "{'loss': 0.4811, 'grad_norm': 0.3156989514827728, 'learning_rate': 4.893617021276596e-06, 'epoch': 0.09}\n",
      "{'loss': 0.467, 'grad_norm': 0.24281872808933258, 'learning_rate': 2.7659574468085106e-06, 'epoch': 0.1}\n",
      "{'loss': 0.4622, 'grad_norm': 0.24942170083522797, 'learning_rate': 6.382978723404255e-07, 'epoch': 0.1}\n",
      "{'train_runtime': 2815.9801, 'train_samples_per_second': 2.841, 'train_steps_per_second': 0.178, 'train_loss': 0.9622677526473999, 'epoch': 0.1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.9622677526473999, metrics={'train_runtime': 2815.9801, 'train_samples_per_second': 2.841, 'train_steps_per_second': 0.178, 'total_flos': 1.01550554873856e+17, 'train_loss': 0.9622677526473999, 'epoch': 0.09713924911360435})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Define a data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=\"longest\",\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=train_args,\n",
    "    peft_config=lora_config,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=data_collator,\n",
    "    max_seq_length=1024,  # Adjusted for efficiency\n",
    "    packing=False\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"gemma-2-2b-tr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emre570/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/emre570/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Index put requires the source and destination dtypes match, got Half for the destination and Float for the source.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {key: value\u001b[38;5;241m.\u001b[39mhalf() \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat \u001b[38;5;28;01melse\u001b[39;00m value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Generate response\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Decode and print the output\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "File \u001b[0;32m~/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/llm-works/llmenv/lib/python3.10/site-packages/transformers/generation/utils.py:2215\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2207\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2208\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2209\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2210\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2211\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2212\u001b[0m     )\n\u001b[1;32m   2214\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2215\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2216\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2220\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2226\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2227\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2228\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2229\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2234\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2235\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/llm-works/llmenv/lib/python3.10/site-packages/transformers/generation/utils.py:3206\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3203\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3205\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 3206\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3208\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3209\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3210\u001b[0m     outputs,\n\u001b[1;32m   3211\u001b[0m     model_kwargs,\n\u001b[1;32m   3212\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3213\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/llm-works/llmenv/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/Documents/llm-works/llmenv/lib/python3.10/site-packages/transformers/models/gemma2/modeling_gemma2.py:1049\u001b[0m, in \u001b[0;36mGemma2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1049\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1059\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1062\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/llm-works/llmenv/lib/python3.10/site-packages/transformers/models/gemma2/modeling_gemma2.py:826\u001b[0m, in \u001b[0;36mGemma2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    823\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m--> 826\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gradient_checkpointing_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    837\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[1;32m    838\u001b[0m         hidden_states,\n\u001b[1;32m    839\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    844\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    845\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/_compile.py:32\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[1;32m     30\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m prior \u001b[38;5;241m=\u001b[39m _maybe_set_eval_frame(callback)\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    634\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[0;32m~/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/utils/checkpoint.py:489\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m context_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m noop_context_fn \u001b[38;5;129;01mor\u001b[39;00m debug \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    485\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    486\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing `context_fn` or `debug` is only supported when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    487\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_reentrant=False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    488\u001b[0m         )\n\u001b[0;32m--> 489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCheckpointFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    491\u001b[0m     gen \u001b[38;5;241m=\u001b[39m _checkpoint_without_reentrant_generator(\n\u001b[1;32m    492\u001b[0m         function, preserve, context_fn, determinism_check, debug, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    493\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/autograd/function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/utils/checkpoint.py:264\u001b[0m, in \u001b[0;36mCheckpointFunction.forward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    261\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(\u001b[38;5;241m*\u001b[39mtensor_inputs)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 264\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mrun_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/llm-works/llmenv/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/Documents/llm-works/llmenv/lib/python3.10/site-packages/transformers/models/gemma2/modeling_gemma2.py:553\u001b[0m, in \u001b[0;36mGemma2DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    550\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    552\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 553\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[1;32m    563\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/llm-works/llmenv/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/Documents/llm-works/llmenv/lib/python3.10/site-packages/transformers/models/gemma2/modeling_gemma2.py:239\u001b[0m, in \u001b[0;36mGemma2Attention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;66;03m# sin and cos are specific to RoPE models; cache_position needed for the static cache\u001b[39;00m\n\u001b[1;32m    233\u001b[0m     cache_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    234\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msin\u001b[39m\u001b[38;5;124m\"\u001b[39m: sin,\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcos\u001b[39m\u001b[38;5;124m\"\u001b[39m: cos,\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msliding_window\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msliding_window,\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_position\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_position,\n\u001b[1;32m    238\u001b[0m     }\n\u001b[0;32m--> 239\u001b[0m     key_states, value_states \u001b[38;5;241m=\u001b[39m \u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m key_states \u001b[38;5;241m=\u001b[39m repeat_kv(key_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_groups)\n\u001b[1;32m    242\u001b[0m value_states \u001b[38;5;241m=\u001b[39m repeat_kv(value_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_groups)\n",
      "File \u001b[0;32m~/Documents/llm-works/llmenv/lib/python3.10/site-packages/transformers/cache_utils.py:1713\u001b[0m, in \u001b[0;36mHybridCache.update\u001b[0;34m(self, key_states, value_states, layer_idx, cache_kwargs)\u001b[0m\n\u001b[1;32m   1710\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1711\u001b[0m     update_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_static_update\n\u001b[0;32m-> 1713\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mupdate_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mv_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk_out\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1721\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/llm-works/llmenv/lib/python3.10/site-packages/transformers/cache_utils.py:1679\u001b[0m, in \u001b[0;36mHybridCache._sliding_update\u001b[0;34m(self, cache_position, layer_idx, key_states, value_states, k_out, v_out, max_cache_len)\u001b[0m\n\u001b[1;32m   1676\u001b[0m k_out \u001b[38;5;241m=\u001b[39m k_out[:, :, indices]\n\u001b[1;32m   1677\u001b[0m v_out \u001b[38;5;241m=\u001b[39m v_out[:, :, indices]\n\u001b[0;32m-> 1679\u001b[0m \u001b[43mk_out\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m key_states\n\u001b[1;32m   1680\u001b[0m v_out[:, :, cache_position] \u001b[38;5;241m=\u001b[39m value_states\n\u001b[1;32m   1681\u001b[0m \u001b[38;5;66;03m# `_.zero()` followed by `+=` is equivalent `=`, but compile-friendly (without graph breaks due to assignment)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Index put requires the source and destination dtypes match, got Half for the destination and Float for the source."
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"<start_of_turn>user Sorunun cevabını doğru şekilde açıklar mısın?: Bir elmanın yarısı kaç eder?<end_of_turn><start_of_turn>model \",\n",
    "    \"<start_of_turn>user Bir metni İngilizceye çevir: Bugün hava çok güzel.<end_of_turn><start_of_turn>model \",\n",
    "    \"<start_of_turn>user Kendini tanıt ve ardından bana Türkçe öğrenmek için önerilerde bulun.<end_of_turn><start_of_turn>model \",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    # Tokenize the question\n",
    "    inputs = tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    \n",
    "    # Convert only floating-point tensors (like attention_mask) to FP16\n",
    "    inputs = {key: value.half() if value.dtype == torch.float else value for key, value in inputs.items()}\n",
    "    \n",
    "    # Generate response\n",
    "    outputs = model.generate(**inputs, max_new_tokens=64)\n",
    "    \n",
    "    # Decode and print the output\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 9690815,
     "sourceId": 85416,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 76277,
     "modelInstanceId": 72253,
     "sourceId": 104625,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
