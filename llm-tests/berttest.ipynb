{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Teşekkürler',\n",
       "  'Bu hikayeyi yazma sürecinde yanımda olan sınıf arkadaşlarıma, ',\n",
       "  'Matematik Hocam Şükrü Tekin’e, ',\n",
       "  'Bana fazlasıyla fikir veren Elif ve Nisa’ya,',\n",
       "  'Her daim destek çıkan Semra, Ravza, Nisa ve Elif’e']}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "datasetPath = '../datasets/book-dataset/book-data.txt'\n",
    "rawtextDataset = load_dataset(\"text\", data_files=datasetPath)\n",
    "rawtextDataset[\"train\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Teşekkürler',\n",
       "  'Bu hikayeyi yazma sürecinde yanımda olan sınıf arkadaşlarıma,',\n",
       "  'Matematik Hocam Şükrü Tekin’e,',\n",
       "  'Bana fazlasıyla fikir veren Elif ve Nisa’ya,',\n",
       "  'Her daim destek çıkan Semra, Ravza, Nisa ve Elif’e']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_empty_lines(batch):\n",
    "    return {\"text\": [line for line in batch[\"text\"] if line.strip()]}\n",
    "def remove_empty_lines_sentence(batch):\n",
    "    return {\"text\": [line.strip() for line in batch[\"text\"]]}\n",
    "\n",
    "filteredDataset = rawtextDataset.map(remove_empty_lines, batched=True)\n",
    "filteredDataset = filteredDataset.map(remove_empty_lines_sentence, batched=True)\n",
    "filteredDataset[\"train\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 291\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 33\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitDataset = filteredDataset[\"train\"].train_test_split(test_size=0.1)\n",
    "\n",
    "tokenizedDataset = DatasetDict({\n",
    "    \"train\": splitDataset[\"train\"],\n",
    "    \"validation\": splitDataset[\"test\"]\n",
    "})\n",
    "tokenizedDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd202e580c5149959c6e58b92b07e706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/291 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa48552aa2ac45ffa85fb86743adc790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/33 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 291\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 33\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenizeFunction(textFile):\n",
    "    return tokenizer(textFile[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenizedDataset = tokenizedDataset.map(tokenizeFunction, batched=True)\n",
    "dataCollator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "tokenizedDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 291\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 33\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizedDataset.set_format(\"torch\")\n",
    "tokenizedDataset.column_names\n",
    "tokenizedDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  1054,  1024,  ...,     0,     0,     0],\n",
       "        [  101,  2521,  2175,  ...,     0,     0,     0],\n",
       "        [  101,  1055,  1024,  ...,     0,     0,     0],\n",
       "        [  101,  8915,  3366,  ...,     0,     0,     0],\n",
       "        [  101,  9092, 11722,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizedDataset[\"validation\"][:5][\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text', 'input_ids', 'token_type_ids', 'attention_mask']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizedDataset[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Burada tek kalmamak için üç tane kedi sahiplendim. Linda, Lina ve Mina. Linda British, Lina Smokin, Mina ise Ankara kedisiydi. O kadar uysallardı ki hemen birbirimize alışmıştık. Şimdi yeni evimde neredeyse tüm şehri gören balkonumda etrafı izlemeye devam edeyim. Erken uyumam gerekiyordu çünkü yarın ilk iş günümdü. Erkenden yatıp yarın işime gidecektim.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizedDataset[\"train\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x2227fad6ce0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "trainDataLoader = DataLoader(tokenizedDataset[\"train\"], shuffle=True, batch_size=8, collate_fn=dataCollator)\n",
    "valDataLoader = DataLoader(tokenizedDataset[\"validation\"], batch_size=8, collate_fn=dataCollator)\n",
    "\n",
    "\"\"\"\n",
    "for batch in trainDataLoader:\n",
    "    break\n",
    "{k: v.shape for k, v in batch.items()}\"\"\"\n",
    "\n",
    "trainDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
