{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":64148,"databundleVersionId":7669720,"sourceType":"competition"},{"sourceId":11394,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":8332}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Gemma 7B Instruct QLoRA Fine Tune","metadata":{}},{"cell_type":"markdown","source":"This Gemma model fine-tuned in another enviroment, then imported to Kaggle. The first cell below gets the necessary files from Kaggle. You can ignore it if you use a notebook from Kaggle.","metadata":{}},{"cell_type":"code","source":"\n# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n# THEN FEEL FREE TO DELETE THIS CELL.\n# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n# NOTEBOOK.\n\nimport os\nimport sys\nfrom tempfile import NamedTemporaryFile\nfrom urllib.request import urlopen\nfrom urllib.parse import unquote, urlparse\nfrom urllib.error import HTTPError\nfrom zipfile import ZipFile\nimport tarfile\nimport shutil\n\nCHUNK_SIZE = 40960\nDATA_SOURCE_MAPPING = 'data-assistants-with-gemma:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F64148%2F7669720%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240304%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240304T124356Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D0e42726848e5dc421f4806827168c3dfc45eee2b9cf8d9c75009b47ba661c2c3729adbee6fa9982575c2df637c85289738b1b9f23b7a01531c11f3866d34d1c250b524800465b969c23a6da7951c81ebac407a6b8d79ab7f8077f21ca35351f368d4f63c60d0d1264ad0ef9012fa6822654b904f4ac3b3668a7c580e3b5606a755744cee24a58847957801c2756d4c758441bcab47245b059b36535eb909abf9cb2ec3f471f32086df0bb530ef7df24223b285a9a2c58d8eddfc2dd8fe4c8e603ed075c737c1ee84afde9992550964a3369c935cd847c2bdb58e3273e51487868a3e0a8fd29a2e31ec0d1928d7cd80e83ba0214fd89fab9fe046c94bc59afbc0,gemma/transformers/7b-it/2:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F8332%2F11394%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240304%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240304T124357Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D84791282594a647fdb1f698f45c8882e474209f32b51a2aa31b6b7afb8f65802f9ef908dc620970288041755bf709b5802c91ccb31022c6fe04ca2c166c75cfc551eaea498a2bad83a06e26c7cd5cbdfa626e8edd470658784704a71721fd12e970a166d22dd6762150212f772cf3788d47719aabcc00919e0af841fd8f3c8d62e96ccffba9b6b9aff0213b9fd071a44ec318631fae32a7f444b2a26d64a3038e7ca60301088e18d7df57111d236b19f54681c3a1e3990850b906af9e687196fe0b3e2faad4e046302176d9944a52c7ec13e1ad65819af723f67a85e21b2a969ebdeefda8fbb995b5ce9632411a5fbecf8d240fe1536e2dba6e307e045c9b587'\n\nKAGGLE_INPUT_PATH='/kaggle/input'\nKAGGLE_WORKING_PATH='/kaggle/working'\nKAGGLE_SYMLINK='kaggle'\n\n!umount /kaggle/input/ 2> /dev/null\nshutil.rmtree('/kaggle/input', ignore_errors=True)\nos.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\nos.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n\ntry:\n  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\nexcept FileExistsError:\n  pass\ntry:\n  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\nexcept FileExistsError:\n  pass\n\nfor data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n    directory, download_url_encoded = data_source_mapping.split(':')\n    download_url = unquote(download_url_encoded)\n    filename = urlparse(download_url).path\n    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n    try:\n        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n            total_length = fileres.headers['content-length']\n            print(f'Downloading {directory}, {total_length} bytes compressed')\n            dl = 0\n            data = fileres.read(CHUNK_SIZE)\n            while len(data) > 0:\n                dl += len(data)\n                tfile.write(data)\n                done = int(50 * dl / int(total_length))\n                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n                sys.stdout.flush()\n                data = fileres.read(CHUNK_SIZE)\n            if filename.endswith('.zip'):\n              with ZipFile(tfile) as zfile:\n                zfile.extractall(destination_path)\n            else:\n              with tarfile.open(tfile.name) as tarfile:\n                tarfile.extractall(destination_path)\n            print(f'\\nDownloaded and uncompressed: {directory}')\n    except HTTPError as e:\n        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n        continue\n    except OSError as e:\n        print(f'Failed to load {download_url} to path {destination_path}')\n        continue\n\nprint('Data source import complete.')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### First, we install necessary libraries for fine-tuning, and datasets.","metadata":{}},{"cell_type":"code","source":"!pip install transformers datasets accelerate peft trl bitsandbytes wandb","metadata":{},"execution_count":2,"outputs":[{"name":"stdout","output_type":"stream","text":"Requirement already satisfied: transformers in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (4.37.2)\n\nCollecting datasets\n\n  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n\nCollecting accelerate\n\n  Downloading accelerate-0.27.2-py3-none-any.whl.metadata (18 kB)\n\nCollecting peft\n\n  Downloading peft-0.9.0-py3-none-any.whl.metadata (13 kB)\n\nCollecting trl\n\n  Downloading trl-0.7.11-py3-none-any.whl.metadata (10 kB)\n\nCollecting bitsandbytes\n\n  Downloading bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\n\nCollecting wandb\n\n  Downloading wandb-0.16.3-py3-none-any.whl.metadata (9.9 kB)\n\nRequirement already satisfied: filelock in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers) (3.13.1)\n\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers) (0.20.3)\n\nRequirement already satisfied: numpy>=1.17 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers) (1.26.3)\n\nRequirement already satisfied: packaging>=20.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers) (23.2)\n\nRequirement already satisfied: pyyaml>=5.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers) (6.0.1)\n\nRequirement already satisfied: regex!=2019.12.17 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers) (2023.12.25)\n\nRequirement already satisfied: requests in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers) (2.31.0)\n\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers) (0.15.2)\n\nRequirement already satisfied: safetensors>=0.4.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers) (0.4.2)\n\nRequirement already satisfied: tqdm>=4.27 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers) (4.66.2)\n\nCollecting pyarrow>=12.0.0 (from datasets)\n\n  Downloading pyarrow-15.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n\nCollecting pyarrow-hotfix (from datasets)\n\n  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n\nCollecting dill<0.3.9,>=0.3.0 (from datasets)\n\n  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n\nRequirement already satisfied: pandas in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (2.2.0)\n\nCollecting xxhash (from datasets)\n\n  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n\nCollecting multiprocess (from datasets)\n\n  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n\nRequirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2023.12.2)\n\nCollecting aiohttp (from datasets)\n\n  Downloading aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n\nRequirement already satisfied: psutil in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from accelerate) (5.9.8)\n\nRequirement already satisfied: torch>=1.10.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from accelerate) (2.1.2)\n\nCollecting tyro>=0.5.11 (from trl)\n\n  Downloading tyro-0.7.3-py3-none-any.whl.metadata (7.7 kB)\n\nCollecting scipy (from bitsandbytes)\n\n  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m799.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\n\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from wandb) (8.1.7)\n\nCollecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n\n  Downloading GitPython-3.1.42-py3-none-any.whl.metadata (12 kB)\n\nCollecting sentry-sdk>=1.0.0 (from wandb)\n\n  Downloading sentry_sdk-1.40.6-py2.py3-none-any.whl.metadata (9.7 kB)\n\nCollecting docker-pycreds>=0.4.0 (from wandb)\n\n  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n\nCollecting setproctitle (from wandb)\n\n  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n\nRequirement already satisfied: setuptools in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from wandb) (68.2.2)\n\nCollecting appdirs>=1.4.3 (from wandb)\n\n  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n\nCollecting protobuf!=4.21.0,<5,>=3.19.0 (from wandb)\n\n  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n\nRequirement already satisfied: six>=1.4.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n\nCollecting aiosignal>=1.1.2 (from aiohttp->datasets)\n\n  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n\nRequirement already satisfied: attrs>=17.3.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n\nCollecting frozenlist>=1.1.1 (from aiohttp->datasets)\n\n  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n\nCollecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n\n  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n\nCollecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n\n  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n\nCollecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n\n  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n\nCollecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n\n  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n\nRequirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n\nRequirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests->transformers) (3.6)\n\nRequirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests->transformers) (2.1.0)\n\nRequirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n\nRequirement already satisfied: sympy in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n\nRequirement already satisfied: networkx in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n\nRequirement already satisfied: jinja2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n\nRequirement already satisfied: nvidia-nccl-cu12==2.18.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.18.1)\n\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n\nRequirement already satisfied: triton==2.1.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.1.0)\n\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.3.101)\n\nCollecting docstring-parser>=0.14.1 (from tyro>=0.5.11->trl)\n\n  Downloading docstring_parser-0.15-py3-none-any.whl.metadata (2.4 kB)\n\nCollecting rich>=11.1.0 (from tyro>=0.5.11->trl)\n\n  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n\nCollecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n\n  Downloading shtab-1.7.0-py3-none-any.whl.metadata (7.3 kB)\n\nRequirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n\nRequirement already satisfied: pytz>=2020.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n\nRequirement already satisfied: tzdata>=2022.7 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n\nCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n\n  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n\nCollecting markdown-it-py>=2.2.0 (from rich>=11.1.0->tyro>=0.5.11->trl)\n\n  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.17.2)\n\nRequirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.4)\n\nRequirement already satisfied: mpmath>=0.19 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n\nCollecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl)\n\n  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n\nDownloading datasets-2.18.0-py3-none-any.whl (510 kB)\n\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\n\u001b[?25hDownloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hDownloading peft-0.9.0-py3-none-any.whl (190 kB)\n\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.9/190.9 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hDownloading trl-0.7.11-py3-none-any.whl (155 kB)\n\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hDownloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\n\u001b[?25hDownloading wandb-0.16.3-py3-none-any.whl (2.2 MB)\n\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\n\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n\nDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n\nDownloading aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n\n\u001b[?25hDownloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hDownloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hDownloading pyarrow-15.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\n\u001b[?25hDownloading sentry_sdk-1.40.6-py2.py3-none-any.whl (258 kB)\n\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.5/258.5 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hDownloading tyro-0.7.3-py3-none-any.whl (79 kB)\n\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n\nDownloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\n\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n\nDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n\nDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n\nDownloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n\nDownloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hDownloading rich-13.7.1-py3-none-any.whl (240 kB)\n\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hDownloading shtab-1.7.0-py3-none-any.whl (14 kB)\n\nDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n\nDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n\nInstalling collected packages: appdirs, xxhash, smmap, shtab, setproctitle, sentry-sdk, scipy, pyarrow-hotfix, pyarrow, protobuf, multidict, mdurl, frozenlist, docstring-parser, docker-pycreds, dill, async-timeout, yarl, multiprocess, markdown-it-py, gitdb, bitsandbytes, aiosignal, rich, GitPython, aiohttp, wandb, tyro, accelerate, peft, datasets, trl\n\nSuccessfully installed GitPython-3.1.42 accelerate-0.27.2 aiohttp-3.9.3 aiosignal-1.3.1 appdirs-1.4.4 async-timeout-4.0.3 bitsandbytes-0.42.0 datasets-2.18.0 dill-0.3.8 docker-pycreds-0.4.0 docstring-parser-0.15 frozenlist-1.4.1 gitdb-4.0.11 markdown-it-py-3.0.0 mdurl-0.1.2 multidict-6.0.5 multiprocess-0.70.16 peft-0.9.0 protobuf-4.25.3 pyarrow-15.0.0 pyarrow-hotfix-0.6 rich-13.7.1 scipy-1.12.0 sentry-sdk-1.40.6 setproctitle-1.3.3 shtab-1.7.0 smmap-5.0.1 trl-0.7.11 tyro-0.7.3 wandb-0.16.3 xxhash-3.4.1 yarl-1.9.4\n\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\n\u001b[0m\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"}]},{"cell_type":"markdown","source":"Hugging Face Transformers and Datasets libraries are already installed in Kaggle Notebooks. However, when I process the dataset, I encountered an error and updating the Datasets library solved the problem. Also, Gemma models comes with latest HF Transformers Library, so updating both Transformers and Datasets is a must.","metadata":{}},{"cell_type":"code","source":"!pip install transformers datasets --upgrade","metadata":{"scrolled":true},"execution_count":4,"outputs":[{"name":"stdout","output_type":"stream","text":"Requirement already satisfied: transformers in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (4.37.2)\n\nCollecting transformers\n\n  Downloading transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.7/130.7 kB\u001b[0m \u001b[31m754.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\n\u001b[?25hRequirement already satisfied: datasets in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (2.18.0)\n\nRequirement already satisfied: filelock in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers) (3.13.1)\n\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers) (0.20.3)\n\nRequirement already satisfied: numpy>=1.17 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers) (1.26.3)\n\nRequirement already satisfied: packaging>=20.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers) (23.2)\n\nRequirement already satisfied: pyyaml>=5.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers) (6.0.1)\n\nRequirement already satisfied: regex!=2019.12.17 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers) (2023.12.25)\n\nRequirement already satisfied: requests in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers) (2.31.0)\n\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers) (0.15.2)\n\nRequirement already satisfied: safetensors>=0.4.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers) (0.4.2)\n\nRequirement already satisfied: tqdm>=4.27 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers) (4.66.2)\n\nRequirement already satisfied: pyarrow>=12.0.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (15.0.0)\n\nRequirement already satisfied: pyarrow-hotfix in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (0.6)\n\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (0.3.8)\n\nRequirement already satisfied: pandas in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (2.2.0)\n\nRequirement already satisfied: xxhash in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (3.4.1)\n\nRequirement already satisfied: multiprocess in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (0.70.16)\n\nRequirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2023.12.2)\n\nRequirement already satisfied: aiohttp in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from datasets) (3.9.3)\n\nRequirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n\nRequirement already satisfied: attrs>=17.3.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n\nRequirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n\nRequirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n\nRequirement already satisfied: yarl<2.0,>=1.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n\nRequirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n\nRequirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests->transformers) (3.6)\n\nRequirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests->transformers) (2.1.0)\n\nRequirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\n\nRequirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n\nRequirement already satisfied: pytz>=2020.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n\nRequirement already satisfied: tzdata>=2022.7 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n\nRequirement already satisfied: six>=1.5 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n\nDownloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\n\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\n\u001b[?25hInstalling collected packages: transformers\n\n  Attempting uninstall: transformers\n\n    Found existing installation: transformers 4.37.2\n\n    Uninstalling transformers-4.37.2:\n\n      Successfully uninstalled transformers-4.37.2\n\nSuccessfully installed transformers-4.38.2\n\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\n\u001b[0m\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"}]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{},"execution_count":5,"outputs":[{"name":"stdout","output_type":"stream","text":"/kaggle/input/data-assistants-with-gemma/submission_instructions.txt\n\n/kaggle/input/data-assistants-with-gemma/submission_categories.txt\n"}]},{"cell_type":"code","source":"import torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Google provides Gemma to users with disclaimer, so you must accept the disclaimer if you want to use Gemma from Hugging Face. Accept it and get a HF Token from your profile and enter it here.","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"944ec8524e4a4edcb120bead9a11237e","version_major":2,"version_minor":0},"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"]},"metadata":{}}]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"Google wants these features answered or explained from Gemma:\n* Data Science Basics and Concepts\n* Python Basics and Concepts\n* Kaggle Basics and Concepts\n\nI did not found any dataset related to Kaggle platform, so I did not add any dataset related to Kaggle.\nI found 2 dataset from HF related to out subject:\n\n* https://huggingface.co/datasets/mlabonne/Evol-Instruct-Python-1k\n* https://huggingface.co/datasets/RazinAleks/SO-Python_QA-Data_Science_and_Machine_Learning_class\n\nEdit: I found this dataset for Kaggle, but it has 1M+ rows so it's HUGE. Maybe you will try :)","metadata":{}},{"cell_type":"markdown","source":"#### Plan for Preprocessing\n\nAs I said, I found 2 datasets. I will combine these 2 datasets as one and fine-tune Gemma with the combined dataset. It seemed more appropriate to me to do this. You can do this process seperated, if you wish.","metadata":{}},{"cell_type":"markdown","source":"Instruct dataset is Python code dataset, QA Dataset is question-answer dataset related to Python and Data Science concepts.","metadata":{}},{"cell_type":"code","source":"import datasets\n\nrawInstructDataset = datasets.load_dataset(\"mlabonne/Evol-Instruct-Python-1k\", split=\"train\")\nrawQADataset = datasets.load_dataset(\"RazinAleks/SO-Python_QA-Data_Science_and_Machine_Learning_class\")\nrawInstructDataset, rawQADataset","metadata":{},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"75ddcf7b3c2e4f668b4426401ca6f51f","version_major":2,"version_minor":0},"text/plain":["Downloading readme:   0%|          | 0.00/756 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4620d4f041884d33beb47e195837b247","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/2.32M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b1c39fe0db7740c7b3e07924a3c875a5","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3bb28b6c74b24825b186c39df999d356","version_major":2,"version_minor":0},"text/plain":["Downloading readme: 0.00B [00:00, ?B/s]"]},"metadata":{}},{"name":"stderr","output_type":"stream","text":"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n\n  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e84282e48b1040739b6677aebfad8864","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/9.90M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ef86da903168441f962700cb051c0eb4","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/2.82M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d2a2091ff8954627984ed14000d7c070","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/1.40M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7714b0847d0349969213758c45a09977","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1c3b5486c580481eb313ae47b9a98191","version_major":2,"version_minor":0},"text/plain":["Generating validation split: 0 examples [00:00, ? examples/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4f0b62b90e484defb33aa80e6fbcbf4b","version_major":2,"version_minor":0},"text/plain":["Generating test split: 0 examples [00:00, ? examples/s]"]},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":["(Dataset({\n","     features: ['instruction', 'output'],\n","     num_rows: 1000\n"," }),\n"," DatasetDict({\n","     train: Dataset({\n","         features: ['ViewCount', 'CreationDate', 'Answer', 'Tags', 'Available Count', 'Q_Score', 'Networking and APIs', 'Q_Id', 'Score', 'Data Science and Machine Learning', 'Database and SQL', 'Other', 'GUI and Desktop Applications', 'Users Score', 'A_Id', 'Title', 'is_accepted', 'AnswerCount', 'Web Development', 'Python Basics and Environment', 'Question', 'System Administration and DevOps'],\n","         num_rows: 6223\n","     })\n","     validation: Dataset({\n","         features: ['ViewCount', 'CreationDate', 'Answer', 'Tags', 'Available Count', 'Q_Score', 'Networking and APIs', 'Q_Id', 'Score', 'Data Science and Machine Learning', 'Database and SQL', 'Other', 'GUI and Desktop Applications', 'Users Score', 'A_Id', 'Title', 'is_accepted', 'AnswerCount', 'Web Development', 'Python Basics and Environment', 'Question', 'System Administration and DevOps'],\n","         num_rows: 1778\n","     })\n","     test: Dataset({\n","         features: ['ViewCount', 'CreationDate', 'Answer', 'Tags', 'Available Count', 'Q_Score', 'Networking and APIs', 'Q_Id', 'Score', 'Data Science and Machine Learning', 'Database and SQL', 'Other', 'GUI and Desktop Applications', 'Users Score', 'A_Id', 'Title', 'is_accepted', 'AnswerCount', 'Web Development', 'Python Basics and Environment', 'Question', 'System Administration and DevOps'],\n","         num_rows: 888\n","     })\n"," }))"]},"metadata":{}}]},{"cell_type":"markdown","source":"Both datasets structures are different. So we must play with them a little bit. I will refactor QA dataset's structure like Instruct dataset. ","metadata":{}},{"cell_type":"markdown","source":"Let's start with Instruct dataset. I will combine question and answer as one column named \"instructions\".","metadata":{}},{"cell_type":"code","source":"instructTexts = []\n\nfor instruction, output in zip(rawInstructDataset[\"instruction\"], rawInstructDataset[\"output\"]):\n    instructText = instruction + output\n    instructTexts.append(instructText)\n    \ninstructDataset = rawInstructDataset\ninstructDataset = instructDataset.add_column(\"instructions\", column=instructTexts)\ninstructDataset = instructDataset.remove_columns([\"instruction\", \"output\"])\ninstructDataset","metadata":{},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['instructions'],\n","    num_rows: 1000\n","})"]},"metadata":{}}]},{"cell_type":"markdown","source":"Continue the same process with QA Dataset, but merge train, test and validation.","metadata":{}},{"cell_type":"code","source":"from datasets import concatenate_datasets\n\ntrainQADataset = rawQADataset[\"train\"]\ntestQADataset = rawQADataset[\"test\"]\nvalidationQADataset = rawQADataset[\"validation\"]\n\nrawCombinedQADataset = concatenate_datasets([trainQADataset, validationQADataset, testQADataset])\nrawCombinedQADataset","metadata":{},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['ViewCount', 'CreationDate', 'Answer', 'Tags', 'Available Count', 'Q_Score', 'Networking and APIs', 'Q_Id', 'Score', 'Data Science and Machine Learning', 'Database and SQL', 'Other', 'GUI and Desktop Applications', 'Users Score', 'A_Id', 'Title', 'is_accepted', 'AnswerCount', 'Web Development', 'Python Basics and Environment', 'Question', 'System Administration and DevOps'],\n","    num_rows: 8889\n","})"]},"metadata":{}}]},{"cell_type":"markdown","source":"There are lots of unnecessary columns. Delete them and combine questions and answers as one column.","metadata":{}},{"cell_type":"code","source":"QATexts = []\n\nfor question, answer in zip(rawCombinedQADataset[\"Question\"], rawCombinedQADataset[\"Answer\"]):\n    qaText = question + answer\n    QATexts.append(qaText)\n    \nQADataset = rawCombinedQADataset\nQADataset = QADataset.add_column(\"instructions\", column=QATexts)\nQADataset = QADataset.remove_columns(['Q_Score', 'Networking and APIs', 'Available Count', 'CreationDate', 'System Administration and DevOps', \n                                      'GUI and Desktop Applications', 'Database and SQL', 'Other', 'ViewCount', 'Score', 'Tags', 'Web Development', \n                                      'Data Science and Machine Learning', 'AnswerCount', 'A_Id', 'Title', 'is_accepted', 'Answer', 'Q_Id', 'Question', \n                                      'Python Basics and Environment', 'Users Score'])\nQADataset","metadata":{},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['instructions'],\n","    num_rows: 8889\n","})"]},"metadata":{}}]},{"cell_type":"markdown","source":"Finally, merge all final datasets as one final dataset.","metadata":{}},{"cell_type":"code","source":"finalDataset = concatenate_datasets([instructDataset, QADataset])\nfinalDataset","metadata":{},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['instructions'],\n","    num_rows: 9889\n","})"]},"metadata":{}}]},{"cell_type":"markdown","source":"# Model Preparation and Fine-Tuning","metadata":{}},{"cell_type":"markdown","source":"Gemma is a big model (especially 7b) for our hardwares. To eliminate this, we use some techniques, such as quantization and LoRA (Low-Rank Adaptation). You can find detailed info [here](https://pytorch.org/blog/finetune-llms/). Let's prepare our both LoRA and quantization configs below:","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig\nfrom transformers import BitsAndBytesConfig\n\nloraConfig = LoraConfig(\n    r = 8,\n    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    task_type=\"CAUSAL_LM\"\n)\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)","metadata":{},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nmodelName = \"google/gemma-7b-it\"\n\ntokenizer = AutoTokenizer.from_pretrained(modelName)\nmodel = AutoModelForCausalLM.from_pretrained(modelName, quantization_config=bnb_config, device_map=\"auto\")","metadata":{},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2f6175bbb8ea4ea892d162e78e75b714","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/2.16k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e81b7a7596804d3e8794ac3d9a37808f","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0059fa0c45ef47c19073e3f54b4629b0","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fab0d67656714f8d9b43ba0104ef8bf0","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/888 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b5b03797ac90450a93c2689b641bc3af","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bd752d3ddd4d427d9ea6ec66c9b2321f","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f2f9b3c135504aafa21fa0972bcbb2c4","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"62200f87c2334b54b5b2e15eb4d978bc","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2a4b845a08ad404dae2068d83b172817","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"51c54f7b50654483a680440b5753c32e","version_major":2,"version_minor":0},"text/plain":["model-00003-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b36309da688c4df4b3e61faf1bcd4ebb","version_major":2,"version_minor":0},"text/plain":["model-00004-of-00004.safetensors:   0%|          | 0.00/2.11G [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c95ba38cb00b401fab89accb36b1211c","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0b78ca2c0e454e93816b524e7830e38c","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"]},"metadata":{}}]},{"cell_type":"markdown","source":"I will use Trainer for training, and used this trainer arguments from Google's Gemma QLoRA fine-tuning script [here](https://huggingface.co/google/gemma-7b/blob/main/examples/example_sft_qlora.py).","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntrainArgs = TrainingArguments(\n    report_to=\"wandb\",\n    output_dir=\"devfiles/results\",\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    optim=\"paged_adamw_32bit\",\n    save_steps=20,\n    logging_steps=20,\n    learning_rate=2e-4,\n    max_grad_norm=0.3,\n    max_steps=100,\n    warmup_ratio=0.03,\n    lr_scheduler_type=\"constant\",\n    gradient_checkpointing=True,\n    fp16=False,\n    bf16=False)","metadata":{},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"NOTE: I tried training Gemma with 2xT4 GPU's, but VRAM (2x16) was not enough. So, I used a RTX A6000 (48GB) for training from another enviroment.","metadata":{}},{"cell_type":"code","source":"from trl import SFTTrainer\n\ntrainer = SFTTrainer(\n    model = model,\n    args = trainArgs,\n    train_dataset = finalDataset,\n    peft_config = loraConfig,\n    packing = True,\n    dataset_text_field = \"instructions\",\n    tokenizer = tokenizer\n)\ntrainer.train()","metadata":{},"execution_count":16,"outputs":[{"name":"stderr","output_type":"stream","text":"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:225: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n\n  warnings.warn(\n"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"47f4b5f326c443a1a1f52507f7c18b17","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{}},{"name":"stderr","output_type":"stream","text":"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:294: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n\n  warnings.warn(\n\n/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n\n  warnings.warn(\n\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"},{"name":"stdin","output_type":"stream","text":"  ········\n"},{"name":"stderr","output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/.netrc\n"},{"output_type":"display_data","data":{"text/html":["Tracking run with wandb version 0.16.3"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":["Run data is saved locally in <code>/home/wandb/run-20240304_152634-0s11l79a</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/emreai/huggingface/runs/0s11l79a' target=\"_blank\">good-dragon-13</a></strong> to <a href='https://wandb.ai/emreai/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":[" View project at <a href='https://wandb.ai/emreai/huggingface' target=\"_blank\">https://wandb.ai/emreai/huggingface</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":[" View run at <a href='https://wandb.ai/emreai/huggingface/runs/0s11l79a' target=\"_blank\">https://wandb.ai/emreai/huggingface/runs/0s11l79a</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"name":"stderr","output_type":"stream","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n\n/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n\n  warnings.warn(\n"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [100/100 1:31:03, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>20</td>\n","      <td>5.838300</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>1.948800</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>1.750600</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>1.668500</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>1.642200</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"name":"stderr","output_type":"stream","text":"/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n\n  warnings.warn(\n\n/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n\n  warnings.warn(\n\n/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n\n  warnings.warn(\n\n/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n\n  warnings.warn(\n"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=100, training_loss=2.5696884536743165, metrics={'train_runtime': 5528.1434, 'train_samples_per_second': 0.289, 'train_steps_per_second': 0.018, 'total_flos': 7.6443656650752e+16, 'train_loss': 2.5696884536743165, 'epoch': 0.44})"]},"metadata":{}}]},{"cell_type":"markdown","source":"Our model has trained, and ready to use. Let's try Gemma with examples:","metadata":{}},{"cell_type":"code","source":"device = \"cuda:0\"\nquestions = [\"How do i create an array and assign values in Python?\", \"What are Hierarchical Bayes models and where are they used?\"]\n\nfor question in questions:\n    inputs = tokenizer(question, return_tensors=\"pt\").to(device)\n    outputs = model.generate(**inputs, max_new_tokens=64)\n    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n\n\n","metadata":{},"execution_count":20,"outputs":[{"name":"stdout","output_type":"stream","text":"How do i create an array and assign values in Python?\n\n\n\nYou can create an array in Python using the square brackets [] and assign values to it using the square brackets [] as well.\n\n\n\nHere is an example:\n\n\n\n```python\n\narr = [1, 2, 3, 4, 5]\n\nprint(arr)\n\n```\n\n\n\nOutput:\n\n\n\n\n\nWhat are Hierarchical Bayes models and where are they used?\n\n\n\nHierarchical Bayes models are a type of Bayesian model that are used to model complex data structures. They are often used in situations where there is a lot of data and the data is not necessarily independent.\n\n\n\nHierarchical Bayes models are often used in situations where there is a lot of data and the data is not necessarily independent.\n"}]},{"cell_type":"markdown","source":"As you can see, model works fine. You can try whatever prompt you want.","metadata":{}},{"cell_type":"markdown","source":"If you want, you can save the model","metadata":{}},{"cell_type":"code","source":"model.save_pretrained(\"devfiles/saved-models\")","metadata":{},"execution_count":21,"outputs":[]}]}