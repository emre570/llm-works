{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":9690815,"sourceId":85416,"sourceType":"competition"},{"isSourceIdPinned":true,"modelId":76277,"modelInstanceId":72253,"sourceId":104625,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30805,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Gemma 2-2B QLoRA Turkish Fine-Tune Guide\n\nThis notebook is a guide to fine-tune Gemma 2-2B model with QLoRA method on a Turkish dataset for Google's Competition.\n\nPrepared by [Emre Albayrak](https://linktr.ee/emre570)","metadata":{}},{"cell_type":"markdown","source":"## Contents\n\nThe following steps will be followed to fine-tune this notebook:\n\n1. Preparation of the necessary environment\n2. Model preparation and application of the QLoRA method\n3. Dataset operations (Pulling, editing)\n4. Fine-tune process\n5. Evaluation of the fine-tuned model","metadata":{}},{"cell_type":"markdown","source":"## 1. Preparation of the necessary environment\n\nIn order to carry out our operations:\n\n* Hugging Face Transformers for model operations,\n* Hugging Face Datasets for dataset operations,\n* Hugging Face bitsandbytes and PEFT for QLoRA transactions,\n* For fine-tuning we need to install TRL and Accelerate libraries.\n\n**NOTE:** For training, you must have a CUDA supported GPU. I used 2x RTX 4090 GPUs for this notebook.\n\nIn Jupyter Notebook we can install the libraries with pip using the `!pip` command. The `--quiet` is there to prevent these libraries from giving any output.","metadata":{}},{"cell_type":"code","source":"# Remove the comment line and start this cell first.\n#!pip install transformers accelerate datasets peft trl bitsandbytes --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T18:08:26.760434Z","iopub.execute_input":"2024-12-23T18:08:26.762310Z","iopub.status.idle":"2024-12-23T18:08:26.767980Z","shell.execute_reply.started":"2024-12-23T18:08:26.762223Z","shell.execute_reply":"2024-12-23T18:08:26.766783Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"If you want to get the Gemma model through Hugging Face, you need to confirm the permission text prepared by Google on the model's page. After getting the permission, you need to get an Access Token from Hugging Face. After getting your token, run this cell and continue.\n\n**NOTE:** If you want to upload the fine-tuned model to Hugging Face, enter a token with `write` property. If not, you can also enter a token with `read`.\n\nYou can also add this model directly to your notebook environment via Kaggle. You can learn the details here:\n\nhttps://www.kaggle.com/models/google/gemma-2","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"hf_token\")","metadata":{"execution":{"iopub.execute_input":"2024-12-21T17:20:28.367452Z","iopub.status.busy":"2024-12-21T17:20:28.366637Z","iopub.status.idle":"2024-12-21T17:20:28.536480Z","shell.execute_reply":"2024-12-21T17:20:28.535792Z","shell.execute_reply.started":"2024-12-21T17:20:28.367413Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(token=hf_token)","metadata":{"execution":{"iopub.execute_input":"2024-12-21T17:20:28.538653Z","iopub.status.busy":"2024-12-21T17:20:28.538018Z","iopub.status.idle":"2024-12-21T17:20:28.923807Z","shell.execute_reply":"2024-12-21T17:20:28.923041Z","shell.execute_reply.started":"2024-12-21T17:20:28.538611Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## 2. Model Preparation\n#### What is QLoRA?\n\nQLoRA, or Quantized Low-Rank Adaptation, is a method for efficiently fine-tuning large-scale language models.\n\nIn the LoRA (Low-Rank Adaptation) approach, low-rank matrices are added to some important weights of the model, so that the model can be customized with fewer parameter changes. QLoRA takes LoRA one step further by quantizing (shrinking) these low-rank matrices, i.e. expressing numerical values in fewer bits. This process both reduces memory usage and increases computational speed.\n\nIn standard fine-tuning without such methods, all the weights of the model are updated, which involves a large number of parameters and increases both the computation time and the amount of memory required.\n\nTo put it very simply, let's take the example of a library:\n\nThink of a language model as a big library. In this library there are many books (the weights of the model) and each book contains information about the language. Normally, when we want to teach something new, we need to replace or update all the books. This takes a lot of time and effort.\n\nThe LoRA method is like adding just a few new books to the library. These new books work together with the old books to make the library better understand and learn new information. But the old books remain the same, only a few new books are added.\n\nThe way these new books are added is special. They are specially designed to use the content of the old books more effectively. In this way, the library retains a wealth of knowledge and can quickly adapt to new information.\n\nSo the new books contain new information referenced from the old books.\n\nIn QLoRA, these books are also added as 'thin' or 'light' books. That is, these books have fewer pages or are written in a simpler language, so the library (model) can read them faster, take up less space and work even more efficiently.","metadata":{}},{"cell_type":"markdown","source":"Now, let's create the config for LoRA and quantization. For the R and Alpha parameters, we can relate them to how strongly the model learns new information.","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig\n\nlora_config = LoraConfig(\n    r=128,\n    lora_alpha=256,\n    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    task_type=\"CAUSAL_LM\",\n)","metadata":{"execution":{"iopub.execute_input":"2024-12-21T17:20:51.557150Z","iopub.status.busy":"2024-12-21T17:20:51.556776Z","iopub.status.idle":"2024-12-21T17:20:51.563857Z","shell.execute_reply":"2024-12-21T17:20:51.563038Z","shell.execute_reply.started":"2024-12-21T17:20:51.557117Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torch\nfrom transformers import BitsAndBytesConfig\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n    #Use fp16 for not supported bf16 hardware\n    #bnb_4bit_compute_dtype=torch.float16\n)","metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"Configs are ready, now let's take the model and quantize it. The `device_map=“auto”` parameter is used to have the model and processes automatically processed by CUDA devices, if any.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\n\n#If you add the model from Kaggle, use this line.\nmodelName = \"/kaggle/input/gemma-2/transformers/gemma-2-2b/2/\"\n\n#If you want to get model from Hugging Face, use this line.\n#modelName = \"google/gemma-2-2b\"\n\ntokenizer = AutoTokenizer.from_pretrained(modelName, token=hf_token)\nmodel = AutoModelForCausalLM.from_pretrained(modelName, \n                                             quantization_config=bnb_config, \n                                             device_map=\"auto\",\n                                             token=hf_token)","metadata":{"execution":{"iopub.execute_input":"2024-12-21T17:20:57.919610Z","iopub.status.busy":"2024-12-21T17:20:57.918853Z","iopub.status.idle":"2024-12-21T17:21:06.295260Z","shell.execute_reply":"2024-12-21T17:21:06.294314Z","shell.execute_reply.started":"2024-12-21T17:20:57.919572Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6e994641ac90444c8a268cce5804c5a0","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"execution_count":4},{"cell_type":"markdown","source":"## 3. Dataset Operations\n\nWe got the model and quantized it. Now let's move on to dataset operations. The dataset is a combination of different Turkish datasets. You can have a look at it if you want:\n\nhttps://huggingface.co/datasets/cenfis/alpaca-turkish-combined","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"cenfis/alpaca-turkish-combined\", split=\"train\")\ndataset, dataset[0]","metadata":{"execution":{"iopub.execute_input":"2024-12-21T17:21:06.304645Z","iopub.status.busy":"2024-12-21T17:21:06.304393Z","iopub.status.idle":"2024-12-21T17:21:08.070696Z","shell.execute_reply":"2024-12-21T17:21:08.069895Z","shell.execute_reply.started":"2024-12-21T17:21:06.304620Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(Dataset({\n","     features: ['input', 'output', 'instruction'],\n","     num_rows: 82353\n"," }),\n"," {'input': '',\n","  'output': \"Fransa'nın başkenti Paris'tir.\",\n","  'instruction': \"Fransa'nın başkenti nedir?\"})"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"execution_count":6},{"cell_type":"markdown","source":"Language models usually use a prompt template. Since the dataset we compiled earlier conforms to the Alpaca Prompt Template, we will modify it slightly. We will adjust our dataset by looking at Google's Gemma Model Card.\n\nThe eos (end of sentence) token is important because if it is not set, the model may do unlimited generation. Since these are available in the tokenizer, we just need to assign them to the variable.\n\nhttps://ai.google.dev/gemma/docs/formatting","metadata":{}},{"cell_type":"code","source":"gemma_prompt = \"\"\"<start_of_turn>user\n{}: {}<end_of_turn>\n<start_of_turn>model\n{}<end_of_turn>\"\"\"\ngemma_prompt","metadata":{"execution":{"iopub.execute_input":"2024-12-21T17:21:08.072757Z","iopub.status.busy":"2024-12-21T17:21:08.072284Z","iopub.status.idle":"2024-12-21T17:21:08.078120Z","shell.execute_reply":"2024-12-21T17:21:08.077164Z","shell.execute_reply.started":"2024-12-21T17:21:08.072726Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'<start_of_turn>user\\n{}: {}<end_of_turn>\\n<start_of_turn>model\\n{}<end_of_turn>'"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"execution_count":7},{"cell_type":"code","source":"eos_token = tokenizer.eos_token\npad_token = tokenizer.pad_token\ntokenizer.padding_side = \"right\"\n\neos_token, pad_token","metadata":{"execution":{"iopub.execute_input":"2024-12-21T17:21:08.079467Z","iopub.status.busy":"2024-12-21T17:21:08.079166Z","iopub.status.idle":"2024-12-21T17:21:08.086899Z","shell.execute_reply":"2024-12-21T17:21:08.086044Z","shell.execute_reply.started":"2024-12-21T17:21:08.079441Z"},"trusted":true},"outputs":[{"data":{"text/plain":["('<eos>', '<pad>')"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"execution_count":8},{"cell_type":"markdown","source":"Let's define the function that places the data in the dataset in the relevant places in the Gemma Template and apply it on the dataset:","metadata":{}},{"cell_type":"code","source":"def formatting_prompts_func(examples):\n    instructions = examples[\"instruction\"]\n    inputs       = examples[\"input\"]\n    outputs      = examples[\"output\"]\n    texts = []\n    for instruction, input, output in zip(instructions, inputs, outputs):\n        text = gemma_prompt.format(instruction, input, output) + eos_token\n        texts.append(text)\n    return { \"text\" : texts, }\npass","metadata":{"execution":{"iopub.execute_input":"2024-12-21T17:21:08.088469Z","iopub.status.busy":"2024-12-21T17:21:08.088105Z","iopub.status.idle":"2024-12-21T17:21:08.094678Z","shell.execute_reply":"2024-12-21T17:21:08.093954Z","shell.execute_reply.started":"2024-12-21T17:21:08.088432Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"dataset = dataset.map(formatting_prompts_func, batched = True)\ndataset","metadata":{"execution":{"iopub.execute_input":"2024-12-21T17:21:08.095977Z","iopub.status.busy":"2024-12-21T17:21:08.095742Z","iopub.status.idle":"2024-12-21T17:21:08.112613Z","shell.execute_reply":"2024-12-21T17:21:08.111789Z","shell.execute_reply.started":"2024-12-21T17:21:08.095953Z"},"trusted":true},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['input', 'output', 'instruction', 'text'],\n","    num_rows: 82353\n","})"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"execution_count":10},{"cell_type":"code","source":"print(dataset[\"text\"][2])","metadata":{"execution":{"iopub.execute_input":"2024-12-21T17:21:08.114453Z","iopub.status.busy":"2024-12-21T17:21:08.113650Z","iopub.status.idle":"2024-12-21T17:21:08.347105Z","shell.execute_reply":"2024-12-21T17:21:08.346140Z","shell.execute_reply.started":"2024-12-21T17:21:08.114412Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<start_of_turn>user\n","Tek farklı olanı belirleyin.: Twitter, Instagram, Telegram<end_of_turn>\n","<start_of_turn>model\n","Telegram<end_of_turn><eos>\n"]}],"execution_count":11},{"cell_type":"markdown","source":"Before we give our modified dataset directly, let's tokenize it with our model's tokenizer and continue.","metadata":{}},{"cell_type":"code","source":"def tokenize_function(examples):\n    tokenized = tokenizer(\n        examples[\"text\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=1024,\n        return_tensors=\"pt\"\n    )\n    # Labels are identical to input_ids for causal language modeling\n    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n    return tokenized\n\nprint(\"Tokenizing dataset...\")\ndataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\nprint(\"Dataset tokenized:\", dataset[0])","metadata":{"execution":{"iopub.execute_input":"2024-12-21T17:21:08.348611Z","iopub.status.busy":"2024-12-21T17:21:08.348283Z","iopub.status.idle":"2024-12-21T17:21:08.737064Z","shell.execute_reply":"2024-12-21T17:21:08.736175Z","shell.execute_reply.started":"2024-12-21T17:21:08.348583Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokenizing dataset...\n","Dataset tokenized: {'input': '', 'output': \"Fransa'nın başkenti Paris'tir.\", 'instruction': \"Fransa'nın başkenti nedir?\", 'input_ids': [2, 106, 1645, 108, 21727, 29541, 235303, 68749, 20074, 235273, 1077, 91278, 7846, 235248, 107, 108, 106, 2516, 108, 21727, 29541, 235303, 68749, 20074, 235273, 1077, 7127, 235303, 6651, 235265, 107, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [2, 106, 1645, 108, 21727, 29541, 235303, 68749, 20074, 235273, 1077, 91278, 7846, 235248, 107, 108, 106, 2516, 108, 21727, 29541, 235303, 68749, 20074, 235273, 1077, 7127, 235303, 6651, 235265, 107, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"]}],"execution_count":12},{"cell_type":"markdown","source":"Looks good, let's continue to fine-tuning.","metadata":{}},{"cell_type":"markdown","source":"## 4. Fine-tune Operations\n\nSFTTrainer (Sparse Fine-Tuning Trainer) is a customized version of Trainer and is designed for a specific type of fine-tuning, i.e. “sparse fine-tuning”. This way only a small part of the model is updated.\n\nLet's define the parameters we will use for fine-tuning and then start the process with SFTTrainer:","metadata":{}},{"cell_type":"markdown","source":"Before we continue, let's break down something. The precision for training.\n\nIn computing, precision refers to how numbers are represented in a computer. When we deal with floating-point numbers (like 1.23, 0.456), we can use different levels of precision to represent them. Higher precision means more accurate representations, but it also costs more memory and computation.\n\nRegular float numbers, typically referred to as single-precision floating-point numbers (FP32), use 32 bits (4 bytes).\n\nFP16, or 16-bit floating-point, is a compact way to store numbers. It uses 16 bits (binary digits) to represent a number.\n\nBF16, short for bfloat16, is another 16-bit floating-point format. It also uses 16 bits but is structured differently.\n\nBF16 is often preferred on NVIDIA Ampere GPUs like the RTX 4090 because it balances range and precision better than FP16. This choice saves memory while leveraging hardware optimizations. I tried with both fp16 and bf16 with same configs, and bf16 ended up to 50 minutes faster.\n\n![](https://images.contentstack.io/v3/assets/blt71da4c740e00faaa/blt40c8ab571893763a/65f370cc0c744dfa367c0793/EXX-blog-fp64-fp32-fp-16-5_(3).jpg?format=webp)","metadata":{}},{"cell_type":"markdown","source":"Also note that we use `max_steps` instead of `epochs`. We show our model a certain amount of dataset. 1 epoch means our model saw all dataset once. You can tweak them as you like but I kept it short for timing.","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntrain_args = TrainingArguments(\n    per_device_train_batch_size=4,  # Each GPU processes 4 examples per step.\n    gradient_accumulation_steps=4,  # Gradients are accumulated over 4 steps before updating weights.\n    warmup_steps=30,  # Learning rate warms up (gradually increases) for the first 30 steps.\n    max_steps=2500,  # Total number of optimization steps for training.\n    # num_train_epochs=3,  # Not used because `max_steps` defines the training duration.\n    gradient_checkpointing=True,  # Saves memory by recomputing activations during backpropagation.\n    learning_rate=1e-4,  # Base learning rate for the optimizer.\n    fp16=False,  # FP16 precision is disabled (not used).\n    bf16=True,  # Enables bfloat16 precision, optimized for RTX 4090 GPUs.\n    logging_steps=125,  # Logs training metrics every 125 steps.\n    optim=\"adamw_8bit\",  # Uses AdamW optimizer with 8-bit precision for optimizer states to save memory.\n    weight_decay=0.01,  # Regularization to prevent overfitting by penalizing large weights.\n    lr_scheduler_type=\"linear\",  # Linearly decays learning rate after the warmup period.\n    output_dir=\"outputs\",  # Directory where model checkpoints and logs will be saved.\n    report_to=\"none\",  # Disables logging to external tools like TensorBoard or WandB.\n)","metadata":{"execution":{"iopub.execute_input":"2024-12-21T17:21:08.739316Z","iopub.status.busy":"2024-12-21T17:21:08.739047Z","iopub.status.idle":"2024-12-21T17:21:08.784955Z","shell.execute_reply":"2024-12-21T17:21:08.784352Z","shell.execute_reply.started":"2024-12-21T17:21:08.739292Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"### Data Collators\n\nA data collator is a utility in machine learning that processes and prepares batches of data before they are fed into the model during training or evaluation. Specifically, it takes raw samples (e.g., text, labels, etc.) from the dataset and transforms them into a format suitable for the model.","metadata":{}},{"cell_type":"markdown","source":"Let's start the Trainer and let the magic happens :)","metadata":{}},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\nfrom trl import SFTTrainer\n\n# Define a data collator\n# Since we tokenized our dataset and returned them as Torch tensors, you may not need it.\n# If you did not tokenized the dataset, you must use Data Collator. \n#It uses tokenizer, tokenize your training data and returns them as tensors.\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer,\n    model=model,\n    padding=\"longest\",\n    return_tensors=\"pt\"\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    args=train_args,\n    peft_config=lora_config,\n    train_dataset=dataset,\n    data_collator=data_collator,\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.execute_input":"2024-12-21T17:21:10.028882Z","iopub.status.busy":"2024-12-21T17:21:10.028182Z","iopub.status.idle":"2024-12-21T17:21:15.631774Z","shell.execute_reply":"2024-12-21T17:21:15.630116Z","shell.execute_reply.started":"2024-12-21T17:21:10.028847Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_8028/1173343391.py:12: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n","  trainer = SFTTrainer(\n"]},{"name":"stdout","output_type":"stream","text":["[2024-12-23 12:35:08,394] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"]},{"name":"stderr","output_type":"stream","text":["/usr/bin/ld: cannot find -laio: No such file or directory\n","collect2: error: ld returned 1 exit status\n","/usr/bin/ld: cannot find -laio: No such file or directory\n","collect2: error: ld returned 1 exit status\n","`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","/home/emre570/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2500/2500 2:53:20, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>125</td>\n","      <td>2.086600</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>0.504000</td>\n","    </tr>\n","    <tr>\n","      <td>375</td>\n","      <td>0.488600</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.474000</td>\n","    </tr>\n","    <tr>\n","      <td>625</td>\n","      <td>0.465900</td>\n","    </tr>\n","    <tr>\n","      <td>750</td>\n","      <td>0.460500</td>\n","    </tr>\n","    <tr>\n","      <td>875</td>\n","      <td>0.456200</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.459000</td>\n","    </tr>\n","    <tr>\n","      <td>1125</td>\n","      <td>0.450200</td>\n","    </tr>\n","    <tr>\n","      <td>1250</td>\n","      <td>0.447900</td>\n","    </tr>\n","    <tr>\n","      <td>1375</td>\n","      <td>0.449400</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.447200</td>\n","    </tr>\n","    <tr>\n","      <td>1625</td>\n","      <td>0.443800</td>\n","    </tr>\n","    <tr>\n","      <td>1750</td>\n","      <td>0.438000</td>\n","    </tr>\n","    <tr>\n","      <td>1875</td>\n","      <td>0.439500</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.436500</td>\n","    </tr>\n","    <tr>\n","      <td>2125</td>\n","      <td>0.430900</td>\n","    </tr>\n","    <tr>\n","      <td>2250</td>\n","      <td>0.430200</td>\n","    </tr>\n","    <tr>\n","      <td>2375</td>\n","      <td>0.427700</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.428600</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/emre570/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","/home/emre570/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","/home/emre570/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","/home/emre570/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n"]},{"data":{"text/plain":["TrainOutput(global_step=2500, training_loss=0.5332360244750977, metrics={'train_runtime': 10405.0999, 'train_samples_per_second': 3.844, 'train_steps_per_second': 0.24, 'total_flos': 5.3837454901248e+17, 'train_loss': 0.5332360244750977, 'epoch': 0.48569624556802177})"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"execution_count":14},{"cell_type":"code","source":"# You can save the model locally\ntrainer.save_model(\"gemma-2-2b-tr-2500step\")","metadata":{"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Or you can push the model to Hugging Face Hub\nmodel.push_to_hub(\"gemma-2-2b-tr-2500step\", token=hf_token, private=True)\ntokenizer.push_to_hub(\"gemma-2-2b-tr-2500step\", token=hf_token, private=True)","metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dc9a8284e3be427fbde615d46fc0ac81","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/2.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9a28bf332c1a4082a728aaa0def5632c","version_major":2,"version_minor":0},"text/plain":["README.md:   0%|          | 0.00/5.18k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a25a8711e777468dafc108420ba579ce","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/34.4M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/emre570/gemma-2-2b-tr-2500step/commit/69bb957a0fdb71c997f4e5351491138f1f534485', commit_message='Upload tokenizer', commit_description='', oid='69bb957a0fdb71c997f4e5351491138f1f534485', pr_url=None, repo_url=RepoUrl('https://huggingface.co/emre570/gemma-2-2b-tr-2500step', endpoint='https://huggingface.co', repo_type='model', repo_id='emre570/gemma-2-2b-tr-2500step'), pr_revision=None, pr_num=None)"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"execution_count":16},{"cell_type":"markdown","source":"## Model Evaluation\n\nWe trained our model, so let's continue with testing it.\n\nI encountered some problems about evaluation so I found the solution to get the model back again.\n\nIf you don't have enough GPU VRAM, you can restart the notebook and start from there. Do not forget to save your fine-tuned model.","metadata":{}},{"cell_type":"code","source":"from peft import PeftModel\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nbase_model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b\")\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n# Move the model to CUDA device, the code will give device mismatch error if you don't move.\nmodel = PeftModel.from_pretrained(base_model, \"emre570/gemma-2-2b-tr-2500step\").to('cuda')","metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a906911378644a41bcc1d0ea75cfaa1d","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1e1a1043d47e4bb8a36e710bda38940b","version_major":2,"version_minor":0},"text/plain":["adapter_config.json:   0%|          | 0.00/792 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"950061d62f1e4c61a0bd10eed2befa2d","version_major":2,"version_minor":0},"text/plain":["adapter_model.safetensors:   0%|          | 0.00/665M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"execution_count":4},{"cell_type":"code","source":"question = \"Kendini tanıt ve ardından bana Türkçe öğrenmek için önerilerde bulun.\"\n\n# Also move the tokenized question too.\ninputs = tokenizer(question, return_tensors=\"pt\").to('cuda')\n\ngenerated_ids = model.generate(**inputs,\n                              max_new_tokens=128,\n                              do_sample=True,\n                              temperature=1.0,\n                              top_p=0.95,\n                              top_k=50,\n                              repetition_penalty=1.0)\n\nprint(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Kendini tanıt ve ardından bana Türkçe öğrenmek için önerilerde bulun.\n","\n","Önerilerde bulunun.: 1. İyi bilinen mobil uygulamaları kullanın\n","2. Duygusal yorgunluğu en aza indirerek ve kendine sürekli olarak ilgi duyarak\n","3. Duyguları tanımlamak için ipuçları kullanarak\n","4. Duyguları tanımak için kelimelerin içeriklerine dikkat edin\n","5. Anlamak için kullanıcının düşünce sürecinde düşünceyi takip edin\n","6. Düşünce sürecini anlamanın yolları ve kaynakları araştırın\n","7. İnsanlar üzerinde anlamlı etkinlikler yaratmak için etkili stratej\n"]}],"execution_count":18},{"cell_type":"code","source":"question = \"Bir elmanın yarısı kaç eder?\"\n\ninputs = tokenizer(question, return_tensors=\"pt\").to('cuda')\ngenerated_ids = model.generate(**inputs,\n                              max_new_tokens=128,\n                              do_sample=True,\n                              temperature=1.0,\n                              top_p=0.95,\n                              top_k=50,\n                              repetition_penalty=1.5)\nprint(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Bir elmanın yarısı kaç eder?\n","Cevap: 50\n","\n","Elma ikiye kütlesini eşittir. Eğer ikisi de aynı boyuttaysa, birincisi o kadar büyük olduğundan yorucu olmalı ve her biri tam halka bölünecek şekilde kesilmelidir; diğeri ise parçaya ayrılmamış hale geldiği için daha küçük olması gerekir (ayrıca bu konuya aşağıdaki link üzerinden ulaşabilirsiniz). : https://ruedesparadojasintematico/index_files/yaman-olmeden%2Byendirmek-%C3%BCnlk9u-.txt\n","ưỡi ç\n"]}],"execution_count":22},{"cell_type":"markdown","source":"## Conclusion\n\nAs a Turkish native speaker, the model's outputs are not so okay. This may happen because of some reasons as dataset's quality or other reasons. I believe our dataset is fine for most cases, and I think that if we train with more steps or epochs, we can get a more fine model with more reasonable answers. Maybe you can try this. I wanted to save some time and trained with 2500 steps. I may train with one epoch and upload to Hub sometime. \n\nIf you came down to here, please don't forget to upvote this notebook. This notebook is an effort of one week of research. Have a great day :)","metadata":{}}]}