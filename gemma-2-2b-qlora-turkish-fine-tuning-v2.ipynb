{"metadata":{"kernelspec":{"display_name":"llmenv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":9690815,"sourceId":85416,"sourceType":"competition"},{"isSourceIdPinned":true,"modelId":76277,"modelInstanceId":72253,"sourceId":104625,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30805,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/cemalemrealbayrak/gemma-2-2b-qlora-turkish-fine-tuning-updated?scriptVersionId=215660908\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Gemma 2-2B QLoRA Turkish Fine-Tune Guide\n\nThis notebook is a guide to fine-tune Gemma 2-2B model with QLoRA method on a Turkish dataset for Google's Competition.\n\nPrepared by [Emre Albayrak](https://linktr.ee/emre570)","metadata":{}},{"cell_type":"markdown","source":"## Contents\n\nThe following steps will be followed to fine-tune this notebook:\n\n1. Preparation of the necessary environment\n2. Model preparation and application of the QLoRA method\n3. Dataset operations (Pulling, editing)\n4. Fine-tune process\n5. Evaluation of the fine-tuned model","metadata":{}},{"cell_type":"markdown","source":"## 1. Preparation of the necessary environment\n\nIn order to carry out our operations:\n\n* Hugging Face Transformers for model operations,\n* Hugging Face Datasets for dataset operations,\n* Hugging Face bitsandbytes and PEFT for QLoRA transactions,\n* For fine-tuning we need to install TRL and Accelerate libraries.\n\n**NOTE:** For training, you must have a CUDA supported GPU. I used 2x RTX 4090 GPUs for this notebook.\n\nIn Jupyter Notebook we can install the libraries with pip using the `!pip` command. The `--quiet` is there to prevent these libraries from giving any output.","metadata":{}},{"cell_type":"code","source":"# Remove the comment line and start this cell first.\n#!pip install transformers accelerate datasets peft trl bitsandbytes --quiet","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"If you want to get the Gemma model through Hugging Face, you need to confirm the permission text prepared by Google on the model's page. After getting the permission, you need to get an Access Token from Hugging Face. After getting your token, run this cell and continue.\n\n**NOTE:** If you want to upload the fine-tuned model to Hugging Face, enter a token with `write` property. If not, you can also enter a token with `read`.\n\nYou can also add this model directly to your notebook environment via Kaggle. You can learn the details here:\n\nhttps://www.kaggle.com/models/google/gemma-2","metadata":{}},{"cell_type":"code","source":"\"\"\"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"hf_token\")\"\"\"\n\nhf_token = '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'","metadata":{"execution":{"iopub.execute_input":"2024-12-21T17:20:28.367452Z","iopub.status.busy":"2024-12-21T17:20:28.366637Z","iopub.status.idle":"2024-12-21T17:20:28.53648Z","shell.execute_reply":"2024-12-21T17:20:28.535792Z","shell.execute_reply.started":"2024-12-21T17:20:28.367413Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(token=hf_token)","metadata":{"execution":{"iopub.execute_input":"2024-12-21T17:20:28.538653Z","iopub.status.busy":"2024-12-21T17:20:28.538018Z","iopub.status.idle":"2024-12-21T17:20:28.923807Z","shell.execute_reply":"2024-12-21T17:20:28.923041Z","shell.execute_reply.started":"2024-12-21T17:20:28.538611Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## 2. Model Preparation\n#### What is QLoRA?\n\nQLoRA, or Quantized Low-Rank Adaptation, is a method for efficiently fine-tuning large-scale language models.\n\nIn the LoRA (Low-Rank Adaptation) approach, low-rank matrices are added to some important weights of the model, so that the model can be customized with fewer parameter changes. QLoRA takes LoRA one step further by quantizing (shrinking) these low-rank matrices, i.e. expressing numerical values in fewer bits. This process both reduces memory usage and increases computational speed.\n\nIn standard fine-tuning without such methods, all the weights of the model are updated, which involves a large number of parameters and increases both the computation time and the amount of memory required.\n\nTo put it very simply, let's take the example of a library:\n\nThink of a language model as a big library. In this library there are many books (the weights of the model) and each book contains information about the language. Normally, when we want to teach something new, we need to replace or update all the books. This takes a lot of time and effort.\n\nThe LoRA method is like adding just a few new books to the library. These new books work together with the old books to make the library better understand and learn new information. But the old books remain the same, only a few new books are added.\n\nThe way these new books are added is special. They are specially designed to use the content of the old books more effectively. In this way, the library retains a wealth of knowledge and can quickly adapt to new information.\n\nSo the new books contain new information referenced from the old books.\n\nIn QLoRA, these books are also added as 'thin' or 'light' books. That is, these books have fewer pages or are written in a simpler language, so the library (model) can read them faster, take up less space and work even more efficiently.","metadata":{}},{"cell_type":"markdown","source":"Now, let's create the config for LoRA and quantization. For the R and Alpha parameters, we can relate them to how strongly the model learns new information.","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig\n\n\nlora_config = LoraConfig(\n    r=128,\n    lora_alpha=256,\n    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    task_type=\"CAUSAL_LM\",\n)","metadata":{"execution":{"iopub.execute_input":"2024-12-21T17:20:51.55715Z","iopub.status.busy":"2024-12-21T17:20:51.556776Z","iopub.status.idle":"2024-12-21T17:20:51.563857Z","shell.execute_reply":"2024-12-21T17:20:51.563038Z","shell.execute_reply.started":"2024-12-21T17:20:51.557117Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torch\nfrom transformers import BitsAndBytesConfig\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)","metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"Configs are ready, now let's take the model and quantize it. The `device_map=“auto”` parameter is used to have the model and processes automatically processed by CUDA devices, if any.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\n\n#modelName = \"/kaggle/input/gemma-2/transformers/gemma-2-2b/2/\"\nmodelName = \"google/gemma-2-2b\"\n\ntokenizer = AutoTokenizer.from_pretrained(modelName, token=hf_token)\nmodel = AutoModelForCausalLM.from_pretrained(modelName, \n                                             quantization_config=bnb_config, \n                                             device_map=\"auto\",\n                                             token=hf_token)","metadata":{"execution":{"iopub.execute_input":"2024-12-21T17:20:57.91961Z","iopub.status.busy":"2024-12-21T17:20:57.918853Z","iopub.status.idle":"2024-12-21T17:21:06.29526Z","shell.execute_reply":"2024-12-21T17:21:06.294314Z","shell.execute_reply.started":"2024-12-21T17:20:57.919572Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e5c9b84ef8fb4789a94710a343da61bd","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"execution_count":null},{"cell_type":"markdown","source":"## 3. Dataset Operations\n\nWe got the model and quantized it. Now let's move on to dataset operations. The dataset is a combination of different Turkish datasets. You can have a look at it if you want:\n\nhttps://huggingface.co/datasets/cenfis/alpaca-turkish-combined","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"myzens/alpaca-turkish-combined\", split=\"train\")\ndataset, dataset[0]","metadata":{"execution":{"iopub.execute_input":"2024-12-21T17:21:06.304645Z","iopub.status.busy":"2024-12-21T17:21:06.304393Z","iopub.status.idle":"2024-12-21T17:21:08.070696Z","shell.execute_reply":"2024-12-21T17:21:08.069895Z","shell.execute_reply.started":"2024-12-21T17:21:06.30462Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(Dataset({\n","     features: ['input', 'output', 'instruction'],\n","     num_rows: 82353\n"," }),\n"," {'input': '',\n","  'output': \"Fransa'nın başkenti Paris'tir.\",\n","  'instruction': \"Fransa'nın başkenti nedir?\"})"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"execution_count":6},{"cell_type":"markdown","source":"Language models usually use a prompt template. Since the dataset we compiled earlier conforms to the Alpaca Prompt Template, we will modify it slightly. We will adjust our dataset by looking at Google's Gemma Model Card.\n\nThe eos (end of sentence) token is important because if it is not set, the model may do unlimited generation. Since these are available in the tokenizer, we just need to assign them to the variable.\n\nhttps://ai.google.dev/gemma/docs/formatting","metadata":{}},{"cell_type":"code","source":"gemma_prompt = \"\"\"<start_of_turn>user\n{}: {}<end_of_turn>\n<start_of_turn>model\n{}<end_of_turn>\"\"\"\ngemma_prompt","metadata":{"execution":{"iopub.execute_input":"2024-12-21T17:21:08.072757Z","iopub.status.busy":"2024-12-21T17:21:08.072284Z","iopub.status.idle":"2024-12-21T17:21:08.07812Z","shell.execute_reply":"2024-12-21T17:21:08.077164Z","shell.execute_reply.started":"2024-12-21T17:21:08.072726Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'<start_of_turn>user\\n{}: {}<end_of_turn>\\n<start_of_turn>model\\n{}<end_of_turn>'"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"execution_count":7},{"cell_type":"code","source":"eos_token = tokenizer.eos_token\npad_token = tokenizer.pad_token\ntokenizer.padding_side = \"right\"\n\neos_token, pad_token","metadata":{"execution":{"iopub.execute_input":"2024-12-21T17:21:08.079467Z","iopub.status.busy":"2024-12-21T17:21:08.079166Z","iopub.status.idle":"2024-12-21T17:21:08.086899Z","shell.execute_reply":"2024-12-21T17:21:08.086044Z","shell.execute_reply.started":"2024-12-21T17:21:08.079441Z"},"trusted":true},"outputs":[{"data":{"text/plain":["('<eos>', '<pad>')"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"execution_count":8},{"cell_type":"markdown","source":"Let's define the function that places the data in the dataset in the relevant places in the Gemma Template and apply it on the dataset:","metadata":{}},{"cell_type":"code","source":"def formatting_prompts_func(examples):\n    instructions = examples[\"instruction\"]\n    inputs       = examples[\"input\"]\n    outputs      = examples[\"output\"]\n    texts = []\n    for instruction, input, output in zip(instructions, inputs, outputs):\n        text = gemma_prompt.format(instruction, input, output) + eos_token\n        texts.append(text)\n    return { \"text\" : texts, }\npass","metadata":{"execution":{"iopub.execute_input":"2024-12-21T17:21:08.088469Z","iopub.status.busy":"2024-12-21T17:21:08.088105Z","iopub.status.idle":"2024-12-21T17:21:08.094678Z","shell.execute_reply":"2024-12-21T17:21:08.093954Z","shell.execute_reply.started":"2024-12-21T17:21:08.088432Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"dataset = dataset.map(formatting_prompts_func, batched = True)\ndataset","metadata":{"execution":{"iopub.execute_input":"2024-12-21T17:21:08.095977Z","iopub.status.busy":"2024-12-21T17:21:08.095742Z","iopub.status.idle":"2024-12-21T17:21:08.112613Z","shell.execute_reply":"2024-12-21T17:21:08.111789Z","shell.execute_reply.started":"2024-12-21T17:21:08.095953Z"},"trusted":true},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['input', 'output', 'instruction', 'text'],\n","    num_rows: 82353\n","})"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"execution_count":10},{"cell_type":"code","source":"print(dataset[\"text\"][2])","metadata":{"execution":{"iopub.execute_input":"2024-12-21T17:21:08.114453Z","iopub.status.busy":"2024-12-21T17:21:08.11365Z","iopub.status.idle":"2024-12-21T17:21:08.347105Z","shell.execute_reply":"2024-12-21T17:21:08.34614Z","shell.execute_reply.started":"2024-12-21T17:21:08.114412Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<start_of_turn>user\n","Tek farklı olanı belirleyin.: Twitter, Instagram, Telegram<end_of_turn>\n","<start_of_turn>model\n","Telegram<end_of_turn><eos>\n"]}],"execution_count":11},{"cell_type":"markdown","source":"Before we give our modified dataset directly, let's tokenize it with our model's tokenizer and continue.","metadata":{}},{"cell_type":"code","source":"def tokenize_function(examples):\n    tokenized = tokenizer(\n        examples[\"text\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=1024,\n        return_tensors=\"pt\"\n    )\n    # Labels are identical to input_ids for causal language modeling\n    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n    return tokenized\n\nprint(\"Tokenizing dataset...\")\ndataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\nprint(\"Dataset tokenized:\", dataset[0])","metadata":{"execution":{"iopub.execute_input":"2024-12-21T17:21:08.348611Z","iopub.status.busy":"2024-12-21T17:21:08.348283Z","iopub.status.idle":"2024-12-21T17:21:08.737064Z","shell.execute_reply":"2024-12-21T17:21:08.736175Z","shell.execute_reply.started":"2024-12-21T17:21:08.348583Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokenizing dataset...\n","Dataset tokenized: {'input': '', 'output': \"Fransa'nın başkenti Paris'tir.\", 'instruction': \"Fransa'nın başkenti nedir?\", 'input_ids': [2, 106, 1645, 108, 21727, 29541, 235303, 68749, 20074, 235273, 1077, 91278, 7846, 235248, 107, 108, 106, 2516, 108, 21727, 29541, 235303, 68749, 20074, 235273, 1077, 7127, 235303, 6651, 235265, 107, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [2, 106, 1645, 108, 21727, 29541, 235303, 68749, 20074, 235273, 1077, 91278, 7846, 235248, 107, 108, 106, 2516, 108, 21727, 29541, 235303, 68749, 20074, 235273, 1077, 7127, 235303, 6651, 235265, 107, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"]}],"execution_count":12},{"cell_type":"markdown","source":"Looks good, let's continue to fine-tuning.","metadata":{}},{"cell_type":"markdown","source":"## 4. Fine-tune Operations\n\nSFTTrainer (Sparse Fine-Tuning Trainer) is a customized version of Trainer and is designed for a specific type of fine-tuning, i.e. “sparse fine-tuning”. This way only a small part of the model is updated.\n\nLet's define the parameters we will use for fine-tuning and then start the process with SFTTrainer:","metadata":{}},{"cell_type":"markdown","source":"Before we continue, let's break down something. The precision for training.\n\nIn computing, precision refers to how numbers are represented in a computer. When we deal with floating-point numbers (like 1.23, 0.456), we can use different levels of precision to represent them. Higher precision means more accurate representations, but it also costs more memory and computation.\n\nRegular float numbers, typically referred to as single-precision floating-point numbers (FP32), use 32 bits (4 bytes).\n\nFP16, or 16-bit floating-point, is a compact way to store numbers. It uses 16 bits (binary digits) to represent a number.\n\nBF16, short for bfloat16, is another 16-bit floating-point format. It also uses 16 bits but is structured differently.\n\nBF16 is often preferred on NVIDIA Ampere GPUs like the RTX 4090 because it balances range and precision better than FP16. This choice saves memory while leveraging hardware optimizations. I tried with both fp16 and bf16 with same configs, and bf16 ended up to 50 minutes faster.\n\n![](https://images.contentstack.io/v3/assets/blt71da4c740e00faaa/blt40c8ab571893763a/65f370cc0c744dfa367c0793/EXX-blog-fp64-fp32-fp-16-5_(3).jpg?format=webp)","metadata":{}},{"cell_type":"markdown","source":"I used `max_steps` instead of `epochs`. We show our model a certain amount of dataset. 1 epoch means our model saw all dataset once. You can tweak them as you like but I kept it short for timing.\n\n**Update:** I trained the model with 3 epoch again, in order to generate better results.","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntrain_args = TrainingArguments(\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    warmup_steps=30,\n    #max_steps=2500,\n    num_train_epochs=3,\n    gradient_checkpointing=True,\n    learning_rate=1e-4,\n    fp16=False,\n    bf16=True,\n    logging_steps=1000,\n    optim=\"adamw_8bit\",\n    weight_decay=0.01,\n    lr_scheduler_type=\"linear\",\n    output_dir=\"outputs\",    \n    report_to=\"none\",\n)","metadata":{"execution":{"iopub.execute_input":"2024-12-21T17:21:08.739316Z","iopub.status.busy":"2024-12-21T17:21:08.739047Z","iopub.status.idle":"2024-12-21T17:21:08.784955Z","shell.execute_reply":"2024-12-21T17:21:08.784352Z","shell.execute_reply.started":"2024-12-21T17:21:08.739292Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"### Data Collators\n\nA data collator is a utility in machine learning that processes and prepares batches of data before they are fed into the model during training or evaluation. Specifically, it takes raw samples (e.g., text, labels, etc.) from the dataset and transforms them into a format suitable for the model.","metadata":{}},{"cell_type":"markdown","source":"Let's start the Trainer and let the magic happens :)","metadata":{}},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\nfrom trl import SFTTrainer\n\n# Define a data collator\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer,\n    model=model,\n    padding=\"longest\",\n    return_tensors=\"pt\"\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    args=train_args,\n    peft_config=lora_config,\n    train_dataset=dataset,\n    data_collator=data_collator,\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.execute_input":"2024-12-21T17:21:10.028882Z","iopub.status.busy":"2024-12-21T17:21:10.028182Z","iopub.status.idle":"2024-12-21T17:21:15.631774Z","shell.execute_reply":"2024-12-21T17:21:15.630116Z","shell.execute_reply.started":"2024-12-21T17:21:10.028847Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_18186/1173343391.py:12: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n","  trainer = SFTTrainer(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6502f2a77ae243d7b24320c3e956e070","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/15441 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/emre570/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","/home/emre570/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.6427, 'grad_norm': 0.436360627412796, 'learning_rate': 9.370579456232561e-05, 'epoch': 0.19}\n"]},{"name":"stderr","output_type":"stream","text":["/home/emre570/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","/home/emre570/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.4624, 'grad_norm': 0.3329216241836548, 'learning_rate': 8.721692297709429e-05, 'epoch': 0.39}\n"]},{"name":"stderr","output_type":"stream","text":["/home/emre570/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","/home/emre570/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.4439, 'grad_norm': 0.31345340609550476, 'learning_rate': 8.072805139186296e-05, 'epoch': 0.58}\n"]},{"name":"stderr","output_type":"stream","text":["/home/emre570/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","/home/emre570/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.4364, 'grad_norm': 0.31639137864112854, 'learning_rate': 7.423917980663163e-05, 'epoch': 0.78}\n"]},{"name":"stderr","output_type":"stream","text":["/home/emre570/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","/home/emre570/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.4258, 'grad_norm': 0.31105419993400574, 'learning_rate': 6.77503082214003e-05, 'epoch': 0.97}\n"]},{"name":"stderr","output_type":"stream","text":["/home/emre570/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","/home/emre570/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.3872, 'grad_norm': 0.462255597114563, 'learning_rate': 6.126143663616898e-05, 'epoch': 1.17}\n"]},{"name":"stderr","output_type":"stream","text":["/home/emre570/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","/home/emre570/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.382, 'grad_norm': 0.4504398703575134, 'learning_rate': 5.477256505093764e-05, 'epoch': 1.36}\n"]},{"name":"stderr","output_type":"stream","text":["/home/emre570/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","/home/emre570/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.379, 'grad_norm': 0.3415434658527374, 'learning_rate': 4.828369346570631e-05, 'epoch': 1.55}\n"]},{"name":"stderr","output_type":"stream","text":["/home/emre570/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","/home/emre570/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.3743, 'grad_norm': 0.36263391375541687, 'learning_rate': 4.1794821880474986e-05, 'epoch': 1.75}\n"]},{"name":"stderr","output_type":"stream","text":["/home/emre570/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","/home/emre570/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.3706, 'grad_norm': 0.5256677865982056, 'learning_rate': 3.5305950295243654e-05, 'epoch': 1.94}\n"]},{"name":"stderr","output_type":"stream","text":["/home/emre570/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","/home/emre570/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.3308, 'grad_norm': 0.3320375680923462, 'learning_rate': 2.881707871001233e-05, 'epoch': 2.14}\n"]},{"name":"stderr","output_type":"stream","text":["/home/emre570/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","/home/emre570/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.3143, 'grad_norm': 0.4995846152305603, 'learning_rate': 2.2328207124781e-05, 'epoch': 2.33}\n"]},{"name":"stderr","output_type":"stream","text":["/home/emre570/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","/home/emre570/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.3132, 'grad_norm': 0.4464154541492462, 'learning_rate': 1.5839335539549676e-05, 'epoch': 2.53}\n"]},{"name":"stderr","output_type":"stream","text":["/home/emre570/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","/home/emre570/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.3102, 'grad_norm': 0.3397451341152191, 'learning_rate': 9.350463954318345e-06, 'epoch': 2.72}\n"]},{"name":"stderr","output_type":"stream","text":["/home/emre570/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","/home/emre570/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["{'loss': 0.309, 'grad_norm': 0.324847936630249, 'learning_rate': 2.8615923690870157e-06, 'epoch': 2.91}\n"]},{"name":"stderr","output_type":"stream","text":["/home/emre570/Documents/llm-works/llmenv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["{'train_runtime': 69071.4772, 'train_samples_per_second': 3.577, 'train_steps_per_second': 0.224, 'train_loss': 0.38974324931645055, 'epoch': 3.0}\n"]},{"data":{"text/plain":["TrainOutput(global_step=15441, training_loss=0.38974324931645055, metrics={'train_runtime': 69071.4772, 'train_samples_per_second': 3.577, 'train_steps_per_second': 0.224, 'total_flos': 3.324812783608922e+18, 'train_loss': 0.38974324931645055, 'epoch': 2.9995628733789887})"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"execution_count":15},{"cell_type":"code","source":"trainer.save_model(\"gemma-2-2b-tr-3epoch\")","metadata":{"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Or you can push the model to Hugging Face Hub\nmodel.push_to_hub(\"gemma-2-2b-tr-3epoch\", token=hf_token, private=True)\ntokenizer.push_to_hub(\"gemma-2-2b-tr-3epoch\", token=hf_token, private=True)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Evaluation\n\nWe trained our model, so let's continue with testing it.\n\nI encountered some problems about evaluation so I found the solution to get the model back again.\n\nIf you don't have enough GPU VRAM, you can restart the notebook and start from there. Do not forget to save your fine-tuned model.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom peft import PeftModel\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nbase_model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b\")\nmodel = PeftModel.from_pretrained(base_model, \"emre570/gemma-2-2b-tr-3epoch\").to(\"cuda\")\ntokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")","metadata":{"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"09d3d8ed995d40f3b7fa3e28f6ba0169","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9b78fc21e35748d2800dd7c774e0b648","version_major":2,"version_minor":0},"text/plain":["adapter_config.json:   0%|          | 0.00/792 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3297ea38235d4bce94e3195d4ed56ff4","version_major":2,"version_minor":0},"text/plain":["adapter_model.safetensors:   0%|          | 0.00/665M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"execution_count":1},{"cell_type":"code","source":"questions = [\"Verilen konu ile ilgili bir şiir yaz: Bahar\",\n             \"Bir zamanlar küçük bir köyde \",\n             \"Bir üçgenin iç açıları toplamı kaç derecedir?\",\n             \"Mutluluk nedir? Mutluluk, \"]\n\ni = 1\nfor question in questions:\n  inputs = tokenizer(question, return_tensors=\"pt\").to(\"cuda\")\n  outputs = model.generate(**inputs, max_new_tokens=128,\n                          do_sample=True,\n                          temperature=1.0,\n                          top_p=0.95,\n                          top_k=50,\n                          repetition_penalty=1.0)\n  result = f\"\"\"\n  Question {i}:\n  {tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]}\"\"\"\n\n  i+=1\n  print(result)","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","  Question 1:\n","  Verilen konu ile ilgili bir şiir yaz: Bahar\n","model\n","Parlayan güneşin gücüyle,\n","Doğanın çiçek açtığı çağrı,\n","Hafif esinti hızla yaprakları fırlatır,\n","Serin havanın tatlı, tatlı kokusu.\n","\n","Kelebekler güneşin doğuşunda dans eder,\n","Gökyüzünde güzelliği yansıtır,\n","Bulutları karıştırarak hızla geri çekilirler,\n","Çimenlerde tatlı bir koku açar.\n","\n","Çimenler o kadar uzun ve süslenmiş,\n","Sanki canlı olarak parıldarlar,\n","Baharın neşesi her\n","\n","  Question 2:\n","  Bir zamanlar küçük bir köyde 8 yaşındaki bir çocuk yaşarmış. O, ormanda yaşayan efsanevi bir yaratığa dert edildiği için köylüler tarafından korkuluyordu. Umutsuz bir girişimde, çocuk kaçmak için çok uzaklara gitmek zorunda kaldı ve köyü terk etti. Yıllarca seyahat ettikten sonra, çocuk nihayet ailesine kavuştu. Ancak köye geri dönmek için hala çok uzaktaydı ve ailesiyle yeniden bir araya gelme şansını kaçırdığı için pişmanlık duyuyordu.\n","model\n","8 yaşında\n","\n","  Question 3:\n","  Bir üçgenin iç açıları toplamı kaç derecedir? Üçgenin kenar uzunlukları 3, 4 ve 5 olduğuna göre bu soruyu cevaplayın.: \n","model\n","Üçgenin iç açıları toplamı 180 derecedir. Özel açılı üçgenlerin iç açılarının toplamı her zaman 180°'dir; bu, üçgenin üç kenarının birbirine dik olduğu şekliyle bağlantılıdır. Bu durumda üçgenin üç kenarının uzunluğu 3, 4 ve 5 olduğundan, üçgen herhangi bir iki üçgenin iç açıları toplam\n","\n","  Question 4:\n","  Mutluluk nedir? Mutluluk, ışık, neşe, kahkaha, iyi hisler, coşku, sevgi, mutluluk hisleri, şükran, kabul, iç huzur, memnuniyet, iyimserlik ve mutluluk gibi duyguları ifade eder. Tüm bunların hepsi mutlulukla ilişkilidir. Mutluluk, insanın günlük hayatında bulması gereken bir duygu, ışık ve neşe kaynağıdır. Mutluluk, insanın düşüncelerine, duygu ve hislerine bağlı olarak farklı şekillerde kendini gösterebilir. Yani, bireyin mutluluğu herkes için\n"]}],"execution_count":7},{"cell_type":"markdown","source":"## Conclusion (Updated)\n\nAs a Turkish native speaker, the model's outputs were not so okay. This may happen because of some reasons as dataset's quality or other reasons. I believe our dataset is fine for most cases, and I think that if we train with more steps or epochs, we can get a more fine model with more reasonable answers. Maybe you can try this. I wanted to save some time and trained with 2500 steps. I may train with one epoch and upload to Hub sometime. \n\n**Update:** I trained with 3 epochs, and you see the results. They get better.\n\nIf you came down to here, please don't forget to upvote this notebook. This notebook is an effort of one week of research. Have a great day :)","metadata":{}}]}